<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Multi-Camera Vision System for Automated Material Detection and Sorting | Srecharan Selvam </title> <meta name="author" content="Srecharan Selvam"> <meta name="description" content="A real-time computer vision system for material recovery and worker safety monitoring on industrial conveyor belts"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon2.png?f718b847644d9f9675c29095ed121769"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://srecharan.github.io/projects/4_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Srecharan Selvam </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/portfolio/">Portfolio </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Multi-Camera Vision System for Automated Material Detection and Sorting</h1> <p class="post-description">A real-time computer vision system for material recovery and worker safety monitoring on industrial conveyor belts</p> </header> <article> <h3 id="1-overview">1. Overview</h3> <p>A comprehensive real-time computer vision system developed during my engagement at VEE ESS Engineering for enhancing high-value material recovery and worker safety monitoring on industrial conveyor belts. The system combines YOLOv5 for precise material detection and instance segmentation with intelligent background subtraction for motion analysis. The architecture features camera-specific region-of-interest (ROI) processing, worker-interaction filtering to minimize false positives, and a robust counting mechanism. The semi-automation of data collection using Mask R-CNN significantly reduced manual annotation efforts while maintaining high-quality dataset generation.</p> <div style="text-align: center;"> <img src="/assets/img/project-4/sys_pipe.png" alt="System Pipeline" style="width: 100%; max-width: 1000px;"> <p><em>End-to-end system architecture showing the complete pipeline from data collection to deployment</em></p> </div> <hr> <h3 id="2-system-architecture">2. System Architecture</h3> <h4 id="21-data-acquisition-and-processing-pipeline">2.1 Data Acquisition and Processing Pipeline</h4> <p>The system architecture consists of several interconnected components that work together to create a robust, real-time material detection and sorting system:</p> <ul> <li> <strong>Multi-Camera Input</strong>: Multiple cameras monitor different sections of the conveyor belt, providing comprehensive coverage</li> <li> <strong>Parallel Processing Pipelines</strong>: Separate pipelines for material detection and worker safety monitoring</li> <li> <strong>ROI-Based Processing</strong>: Camera-specific regions of interest to focus computational resources on relevant areas</li> <li> <strong>Real-Time Detection System</strong>: YOLOv5-based detection models for materials and workers</li> <li> <strong>False Positive Filtering</strong>: Worker interaction detection to prevent miscounting</li> </ul> <h4 id="22-semi-automated-data-annotation">2.2 Semi-Automated Data Annotation</h4> <p>Creating a robust dataset was one of the key challenges. A multi-stage approach was implemented:</p> <ul> <li> <strong>Initial Dataset Creation</strong>: A small initial dataset of approximately 800 images (200 per material class) was created using: <ul> <li>Manual annotation with LabelMe</li> <li>Traditional computer vision techniques (Canny edge detection, adaptive thresholding)</li> <li>Semi-automated annotation using bounding boxes from a pre-trained object detector</li> </ul> </li> <li> <strong>Mask R-CNN Fine-Tuning</strong>: The initial dataset was used to fine-tune a pre-trained Mask R-CNN model, specifically adapting it to detect and segment the target materials</li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-4/labelme.png" alt="Manual Annotation" style="width: 60%; max-width: 700px;"> <p><em>Initial dataset creation using manual annotation</em></p> </div> <div style="text-align: center;"> <img src="/assets/img/project-4/maskrcnn.drawio.png" alt="Mask R-CNN Architecture" style="width: 100%; max-width: 700px;"> <p><em>Simple Mask R-CNN Architecture Diagram</em></p> </div> <h4 id="23-automated-material-segmentation">2.3 Automated Material Segmentation</h4> <p>Using the fine-tuned Mask R-CNN model, the system could automatically generate segmentation masks for a much larger dataset:</p> <ul> <li> <strong>Automated Mask Generation</strong>: The fine-tuned model processed video feeds through predefined ROIs</li> <li> <strong>Segmented Instance Extraction</strong>: Over 43,000 segmented material instances were automatically extracted</li> <li> <strong>Dataset Expansion</strong>: This approach dramatically improved data collection efficiency</li> </ul> <div style="text-align: center;"> <div style="display: flex; justify-content: center; gap: 20px; margin-bottom: 20px;"> <img src="/assets/img/project-4/trash1.png" alt="Segmented Material 1" style="width: 22%; max-width: 250px;"> <img src="/assets/img/project-4/trash2.png" alt="Segmented Material 2" style="width: 22%; max-width: 250px;"> <img src="/assets/img/project-4/trash3.png" alt="Segmented Material 3" style="width: 22%; max-width: 250px;"> <img src="/assets/img/project-4/trash4.png" alt="Segmented Material 4" style="width: 22%; max-width: 250px;"> </div> <p><em>Individual material instances segmented and extracted from the conveyor belt stream</em></p> </div> <h4 id="24-data-augmentation-strategy">2.4 Data Augmentation Strategy</h4> <p>To create a dataset that generalizes well to real-world conditions, a sophisticated data augmentation strategy was implemented:</p> <ul> <li> <strong>Object-Background Compositing</strong>: Segmented material images were overlaid onto images of empty conveyor belt bins</li> <li> <strong>Multiple Transformations</strong>: Each composite image underwent various transformations: <ul> <li>Rotations to simulate different orientations</li> <li>Scaling to account for size variations</li> <li>Brightness and contrast adjustments to handle lighting changes</li> </ul> </li> <li> <strong>Dual Dataset Creation</strong>: Two distinct datasets were generated: <ul> <li>Detection dataset for YOLOv5 training</li> <li>Segmentation dataset for instance segmentation</li> </ul> </li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-4/get_coord_op.png" alt="ROI Definition" style="width: 60%; max-width: 550px;"> <p><em>Defining the ROI for environmental (bin) extraction for data augmentationt</em></p> </div> <div style="text-align: center;"> <div style="display: flex; justify-content: center; gap: 20px; margin-bottom: 20px;"> <img src="/assets/img/project-4/aug1.jpg" alt="Augmented Data 1" style="width: 23%; max-width: 275px;"> <img src="/assets/img/project-4/aug2.jpg" alt="Augmented Data 2" style="width: 23%; max-width: 275px;"> <img src="/assets/img/project-4/aug3.jpg" alt="Augmented Data 3" style="width: 23%; max-width: 275px;"> <img src="/assets/img/project-4/aug4.jpg" alt="Augmented Data 4" style="width: 23%; max-width: 275px;"> </div> <p><em>Data augmentation showing segmented materials overlaid on bin backgrounds with various transformations</em></p> </div> <hr> <h3 id="3-worker-safety-monitoring">3. Worker Safety Monitoring</h3> <h4 id="31-worker-detection-system">3.1 Worker Detection System</h4> <p>A separate but integrated worker detection system was implemented to ensure worker safety and prevent false positive material counts:</p> <ul> <li> <strong>Color-Based Initial Detection</strong>: A fast color-based detection method identified high-visibility safety vests</li> <li> <strong>YOLOv5 Worker Detection</strong>: A specialized YOLOv5 model was trained for robust worker detection</li> <li> <strong>Bounding Box Generation</strong>: Precise bounding boxes around workers enabled interaction detection</li> </ul> <div style="text-align: center;"> <div style="display: flex; flex-wrap: wrap; justify-content: center; gap: 20px; margin-bottom: 20px;"> <img src="/assets/img/project-4/people_det.jpg" alt="Worker Detection 1" style="width: 45%; max-width: 400px;"> <img src="/assets/img/project-4/people_det2.jpg" alt="Worker Detection 2" style="width: 45%; max-width: 400px;"> <img src="/assets/img/project-4/people_det3.jpg" alt="Worker Detection 3" style="width: 45%; max-width: 400px;"> <img src="/assets/img/project-4/person_det4.jpg" alt="Worker Detection 4" style="width: 45%; max-width: 400px;"> </div> <p><em>Worker detection system identifying safety vest-wearing personnel with precise bounding boxes</em></p> </div> <h4 id="32-false-positive-filtering">3.2 False Positive Filtering</h4> <p>A critical innovation in this system was the ability to prevent false positive material counts when workers interact with the conveyor belt:</p> <ul> <li> <strong>Worker Overlap Detection</strong>: Materials detected within worker bounding boxes are not counted</li> <li> <strong>Temporal Cooldown</strong>: After counting an object, a cooldown period prevents immediate recounting</li> <li> <strong>Background Subtraction</strong>: MOG2 background subtractor further refines motion detection within ROIs</li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-4/false_positves.png" alt="False Positive Filtering" style="width: 100%; max-width: 700px;"> <p><em>Examples of false positives detected and filtered by the system</em></p> </div> <hr> <h3 id="4-real-time-processing">4. Real-Time Processing</h3> <p>The real-time processing system integrates all components to deliver accurate material detection and counting while ensuring worker safety:</p> <ul> <li> <strong>Frame Acquisition</strong>: Continuous frame capture from multiple camera feeds</li> <li> <strong>ROI Processing</strong>: Camera-specific regions are processed separately</li> <li> <strong>Background Subtraction</strong>: Intelligent background subtraction identifies moving objects</li> <li> <strong>False Positive Filtering</strong>: Worker overlap detection prevents miscounting</li> <li> <strong>Material Classification</strong>: Detected materials are classified and counted</li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-4/trash_mask.gif" alt="Real-time Material Detection" style="width: 60%; max-width: 700px;"> <p><em>Real-time segmentation of materials on the conveyor belt using ROI-based detection</em></p> </div> <div style="text-align: center;"> <img src="/assets/img/project-4/data_aug_op.jpg" alt="Real-time Detection Results" style="width: 60%; max-width: 700px;"> <p><em>Real-time detection results showing system performance with detected materials in the production environment</em></p> </div> <hr> <h3 id="5-performance-metrics">5. Performance Metrics</h3> <p>The system achieved exceptional performance across various metrics:</p> <div style="text-align: center;"> <table class="table" style="width: 90%; margin: 0 auto; border-collapse: collapse; border: 1px solid #ddd;"> <thead> <tr style="background-color: #f2f2f2;"> <th style="padding: 12px; border: 1px solid #ddd;">Model</th> <th style="padding: 12px; border: 1px solid #ddd;">mAP50</th> <th style="padding: 12px; border: 1px solid #ddd;">mAP50-95</th> <th style="padding: 12px; border: 1px solid #ddd;">Precision</th> <th style="padding: 12px; border: 1px solid #ddd;">Recall</th> <th style="padding: 12px; border: 1px solid #ddd;">F1 Score</th> </tr> </thead> <tbody> <tr> <td style="padding: 12px; border: 1px solid #ddd;"><strong>Material Detection</strong></td> <td style="padding: 12px; border: 1px solid #ddd;">0.995</td> <td style="padding: 12px; border: 1px solid #ddd;">0.968</td> <td style="padding: 12px; border: 1px solid #ddd;">0.999</td> <td style="padding: 12px; border: 1px solid #ddd;">0.999</td> <td style="padding: 12px; border: 1px solid #ddd;">0.999</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 12px; border: 1px solid #ddd;"><strong>Worker Detection</strong></td> <td style="padding: 12px; border: 1px solid #ddd;">0.977</td> <td style="padding: 12px; border: 1px solid #ddd;">0.745</td> <td style="padding: 12px; border: 1px solid #ddd;">0.938</td> <td style="padding: 12px; border: 1px solid #ddd;">0.960</td> <td style="padding: 12px; border: 1px solid #ddd;">0.950</td> </tr> </tbody> </table> </div> <p style="margin-top: 20px;"> <strong>System Performance</strong><br> • Real-time processing: 30+ FPS<br> • Inference latency: &lt;15ms<br> • False positive rate: &lt;0.5%<br> • Robust to lighting variations and occlusions </p> <hr> <h3 id="6-key-contributions">6. Key Contributions</h3> <p>My key contributions to this project at VEE ESS Engineering included:</p> <ul> <li>Pipeline architecture designed and implemented the end-to-end computer vision pipeline for automated material detection and sorting</li> <li>Semi-automated data collection created a semi-automated data collection and annotation system that reduced manual labeling effort by 90%</li> <li>Worker safety system developed the worker detection and false positive filtering system that increased counting accuracy by 35%</li> <li>ROI management designed the interactive ROI definition system that enabled flexible deployment across different conveyor configurations</li> <li>Model training trained and optimized the YOLOv5 models for both material detection and worker safety monitoring</li> </ul> <hr> <h3 id="7-technologies--skills-used">7. Technologies &amp; Skills Used</h3> <ul> <li> <strong>Languages</strong>: Python, C++</li> <li> <strong>Frameworks</strong>: PyTorch, OpenCV, ROS (Noetic), NumPy, Pandas</li> <li> <strong>Machine Learning</strong>: YOLOv5, Mask R-CNN, Transfer Learning, Data Augmentation</li> <li> <strong>Computer Vision</strong>: Object Detection, Instance Segmentation, Background Subtraction, ROI Processing</li> </ul> <hr> <h3 id="8-project-repository">8. Project Repository</h3> <p><a href="https://github.com/Srecharan/CVAnnotate.git" rel="external nofollow noopener" target="_blank">CVAnnotate</a></p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Srecharan Selvam. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-repositories",title:"Repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-portfolio",title:"Portfolio",description:"A collection of my detailed technical work and projects",section:"Navigation",handler:()=>{window.location.href="/portfolio/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-hybrid-cv-ml-approach-for-autonomous-leaf-grasping",title:"Hybrid CV-ML Approach for Autonomous Leaf Grasping",description:"A novel vision system combining geometric computer vision with deep learning for leaf detection and grasp point optimization",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-deeptrade-ai-multi-model-stock-prediction-with-nlp-amp-automated-trading",title:"DeepTrade AI - Multi-Model Stock Prediction with NLP & Automated Trading",description:"An end-to-end system integrating LSTM-XGBoost prediction with sentiment analysis for automated trading",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-real-time-hand-gesture-recognition-for-ar-interaction",title:"Real-time Hand Gesture Recognition for AR Interaction",description:"A sophisticated system combining computer vision and deep learning for hand tracking and gesture recognition in augmented reality",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-multi-camera-vision-system-for-automated-material-detection-and-sorting",title:"Multi-Camera Vision System for Automated Material Detection and Sorting",description:"A real-time computer vision system for material recovery and worker safety monitoring on industrial conveyor belts",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-deep-image-synthesis-with-gans-vaes-and-diffusion-models",title:"Deep Image Synthesis with GANs, VAEs, and Diffusion Models",description:"A comprehensive implementation of three leading generative model architectures for image synthesis",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-high-fidelity-3d-scene-reconstruction-integrating-diffusion-models-with-memory-efficient-neural-radiance-fields",title:"High-Fidelity 3D Scene Reconstruction Integrating Diffusion Models with Memory-Efficient Neural Radiance Fields",description:"A novel approach combining diffusion models with Neural Radiance Fields for high-quality 3D scene reconstruction",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-toolvisionlm-enhancing-vision-language-models-for-industrial-safety-ongoing",title:"ToolVisionLM: Enhancing Vision-Language Models for Industrial Safety [Ongoing]",description:"A comprehensive evaluation framework for vision-language models in technical domains with focus on industrial tool recognition and safety guidance",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%73%65%6C%76%61%6D@%61%6E%64%72%65%77.%63%6D%75.%65%64%75","_blank")}},{id:"socials-whatsapp",title:"WhatsApp",section:"Socials",handler:()=>{window.open("https://wa.me/19174953410","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=Srecharan Selvam","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Srecharan","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/Srecharan","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>