<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Real-time Hand Gesture Recognition for AR Interaction | Srecharan Selvam </title> <meta name="author" content="Srecharan Selvam"> <meta name="description" content="A sophisticated system combining computer vision and deep learning for hand tracking and gesture recognition in augmented reality"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon2.png?f718b847644d9f9675c29095ed121769"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://srecharan.github.io/projects/3_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Srecharan Selvam </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/portfolio/">Portfolio </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Real-time Hand Gesture Recognition for AR Interaction</h1> <p class="post-description">A sophisticated system combining computer vision and deep learning for hand tracking and gesture recognition in augmented reality</p> </header> <article> <h3 id="1-overview">1. Overview</h3> <p>A sophisticated real-time hand gesture recognition system developed during my internship at Hanon Systems, implementing a hybrid architecture that combines classical computer vision with deep learning approaches. The system leverages depth sensing cameraâ€™s capabilities enhanced by Extended Kalman filtering for precise 3D tracking, while incorporating both MediaPipe-based gesture recognition and optimized ONNX neural network implementations for robust hand detection and pose estimation.</p> <div style="text-align: center;"> <img src="/assets/img/project-3/virtuhand_sys.png" alt="System Architecture" style="width: 3000%; max-width: 800px;"> <p><em> System pipeline showing data flow from camera through Python backend to Unity frontend</em></p> </div> <div style="text-align: center;"> <a href="https://youtu.be/eRFWZjJbcgI" target="_blank" rel="external nofollow noopener"> <img src="/assets/img/project-3/full_demo_gesture_fast.gif" alt="Full Demo" style="width: 100%; max-width: 700px;"> <p><em>ðŸ“Œ For full quality, watch the video on <a href="https://youtu.be/eRFWZjJbcgI" target="_blank" rel="external nofollow noopener">YouTube</a></em></p> </a> </div> <hr> <h3 id="2-hand-detection-and-tracking">2. Hand Detection and Tracking</h3> <h4 id="21-mediapipe-landmark-detection">2.1 MediaPipe Landmark Detection</h4> <p>The system uses Googleâ€™s MediaPipe Hands library as the foundation for initial hand detection and landmark extraction:</p> <ul> <li>Extracts 21 keypoints from each hand in real-time</li> <li>Provides a skeletal representation of hand pose</li> <li>Computationally efficient with high accuracy</li> <li>Enables detection of both left and right hands independently</li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-3/mediapipe_landmarks.png" alt="MediaPipe Landmarks" style="width: 100%; max-width: 800px;"> <p><em>Hand detection with MediaPipe showing 21 landmark points and their connections</em></p> </div> <h4 id="22-3d-tracking-with-extended-kalman-filter">2.2 3D Tracking with Extended Kalman Filter</h4> <p>To achieve precise and stable 3D tracking, the system combines MediaPipe landmarks with depth data and applies Extended Kalman Filtering:</p> <ul> <li> <strong>Multi-stage Depth Filtering Pipeline:</strong> <ul> <li>Spatial filtering to reduce noise</li> <li>Temporal filtering for consistency</li> <li>Kalman filtering for smooth tracking</li> </ul> </li> <li> <strong>Extended Kalman Filter Implementation:</strong> <ul> <li>2D state vector (position, velocity) estimation</li> <li>Optimized noise matrices for hand motion</li> <li>30Hz update rate with dynamic time-step handling</li> <li>Advanced outlier rejection for robust tracking</li> </ul> </li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-3/depth_filter.png" alt="Depth Filtering" style="width: 30%; max-width: 800px;"> <p><em>Visualization of the multi-stage depth filtering process showing raw depth data being transformed into smooth 3D positions</em></p> </div> <div style="text-align: center;"> <img src="/assets/img/project-3/EFK_depth.drawio.png" alt="EKF Flowchart" style="width: 100%; max-width: 800px;"> <p><em>Extended Kalman Filter implementation flowchart showing the prediction-correction cycle</em></p> </div> <h4 id="23-onnx-neural-network-integration">2.3 ONNX Neural Network Integration</h4> <p>The system incorporates an optimized ONNX-based pipeline to improve performance:</p> <ul> <li> <strong>Two-stage Detection System:</strong> <ul> <li>Palm Detection (192x192 input)</li> <li>Hand Landmark Detection (224x224 input)</li> </ul> </li> <li> <strong>Performance Optimizations:</strong> <ul> <li>FP16 quantization reducing model size by 50%</li> <li>Unity Barracuda engine for GPU acceleration</li> <li>Custom tensor preprocessing pipeline</li> <li>Overall inference time &lt;33ms (30+ FPS)</li> </ul> </li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-3/ONNX.png" alt="ONNX Pipeline" style="width: 30%; max-width: 350px;"> <p><em>ONNX neural network pipeline with parallel palm detection and hand landmark models</em></p> </div> <hr> <h3 id="3-gesture-recognition-system">3. Gesture Recognition System</h3> <h4 id="31-static-gesture-recognition">3.1 Static Gesture Recognition</h4> <p>The static gesture recognition system employs geometric analysis of hand landmarks:</p> <ul> <li> <strong>Joint Angle Calculation:</strong> Analysis of angles between finger joints</li> <li> <strong>Finger State Detection:</strong> Adaptive thresholds for open/closed/bent states</li> <li> <strong>Palm Orientation Analysis:</strong> Using normal vectors to determine hand orientation</li> <li> <strong>Real-time Confidence Scoring:</strong> Certainty evaluation for each classification</li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-3/static_gestures.gif" alt="Static Gestures" style="width: 100%; max-width: 700px;"> <p><em>Demonstration of supported static gestures: GRAB, OPEN_PALM, PINCH, and POINT</em></p> </div> <h4 id="32-dynamic-gesture-recognition">3.2 Dynamic Gesture Recognition</h4> <p>For dynamic gestures, the system uses a Gated Recurrent Unit (GRU) neural network combined with custom motion pattern analysis:</p> <ul> <li> <strong>GRU Neural Network:</strong> <ul> <li>Input: Sequence of 30 frames (63 features per frame)</li> <li>Architecture: 2 hidden layers with 32 units each</li> <li>Output: Classification with confidence scores for each dynamic gesture</li> </ul> </li> <li> <strong>Motion Pattern Analysis:</strong> <ul> <li>Velocity component extraction (dx, dy)</li> <li>Horizontal/vertical motion ratio analysis</li> <li>Specialized pattern detectors for SWIPE, CIRCLE, and WAVE gestures</li> </ul> </li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-3/GRU.png" alt="GRU Architecture" style="width: 30%; max-width: 300px;"> <p><em>GRU-based dynamic gesture recognition pipeline with sequence preprocessing and temporal smoothing</em></p> </div> <div style="text-align: center;"> <img src="/assets/img/project-3/dynamic_gestures.gif" alt="Dynamic Gestures" style="width: 100%; max-width: 700px;"> <p><em>Demonstration of supported dynamic gestures: SWIPE_LEFT, SWIPE_RIGHT, and CIRCLE</em></p> </div> <hr> <h3 id="4-unity-integration-and-ar-interaction">4. Unity Integration and AR Interaction</h3> <h4 id="41-real-time-hand-rigging">4.1 Real-time Hand Rigging</h4> <p>The Unity frontend provides visualization and interaction capabilities:</p> <ul> <li> <strong>Hand Model:</strong> Fully articulated 3D hand with 21 joints</li> <li> <strong>Inverse Kinematics:</strong> Realistic hand movement based on tracking data</li> <li> <strong>Real-time Physics:</strong> Dynamic object interaction with collision detection</li> <li> <strong>WebSocket Communication:</strong> Low-latency data streaming between Python backend and Unity frontend</li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-3/hand_rig_2x.gif" alt="Hand Rigging" style="width: 100%; max-width: 700px;"> <p><em>Real-time hand rigging in Unity. The 3D hand model accurately mirrors the user's hand movements and gestures</em></p> </div> <h4 id="42-ar-interaction-demo">4.2 AR Interaction Demo</h4> <p>The system was integrated into an AR environment for demonstration purposes:</p> <ul> <li> <strong>Intuitive Interactions:</strong> Users can grab, move, and place virtual objects</li> <li> <strong>Gesture-Triggered Events:</strong> Different gestures trigger different interactions</li> <li> <strong>Real-time Response:</strong> The system maintains 30+ FPS for seamless user experience</li> </ul> <p style="font-style: italic; font-weight: underline;"> <span style="font-weight: bold; font-style: italic;">Note:</span> The virtual flower arrangement scene shown in the demo was created solely for demonstration purposes. During my internship at Hanon Systems, the actual implementation was focused on enabling automotive technicians to practice precise component placement and assembly procedures for virtual HVAC systems in an automotive manufacturing context. </p> <hr> <h3 id="5-performance-metrics">5. Performance Metrics</h3> <div class="table-responsive"> <table class="table"> <thead> <tr> <th>Component</th> <th>Metric</th> <th>Value</th> </tr> </thead> <tbody> <tr> <td>Hand Tracking</td> <td>Tracking Precision</td> <td>&lt;7.5mm error</td> </tr> <tr> <td>Static Gestures</td> <td>Recognition Accuracy</td> <td>97%</td> </tr> <tr> <td>Dynamic Gestures</td> <td>Recognition Latency</td> <td>&lt;33ms</td> </tr> <tr> <td>ONNX Models</td> <td>Palm Detection Inference</td> <td>8-10ms</td> </tr> <tr> <td>ONNX Models</td> <td>Landmark Detection Inference</td> <td>12-15ms</td> </tr> <tr> <td>Overall System</td> <td>Frame Rate</td> <td>30+ FPS</td> </tr> </tbody> </table> </div> <hr> <h3 id="6-contribution">6. Contribution</h3> <p>During my internship at Hanon Systems, I contributed to the HVAC Systems Simulation Team as a Machine Learning Engineer Intern by architecting and implementing a real-time 3D hand tracking and gesture recognition system for augmented reality (AR) applications within Unity. This system enabled over 50 automotive technicians to practice component placement in virtual HVAC systems.</p> <p>My key contributions included:</p> <ul> <li>Integrating a depth-sensing camera for capturing 3D data and utilizing MediaPipe and an Extended Kalman Filter for robust 3D hand tracking, achieving less than 7.5mm ground truth tracking accuracy</li> <li>Designing and implementing both static gesture recognition (using geometric analysis) and dynamic gesture recognition (using a custom-trained GRU network and motion analysis), achieving 97% accuracy for static gestures and under 30ms latency for dynamic gestures</li> <li>Optimizing the pipeline with ONNX, achieving 33% faster inference with 50% smaller model size than MediaPipe</li> <li>Establishing real-time communication between a Python backend and the Unity AR frontend using WebSockets, enabling a 30Hz data streaming rate with packets under 1KB</li> <li>Implementing the 3D hand model rigging and inverse kinematics for realistic hand movement in the Unity environment</li> </ul> <p>The demonstration scene shown in this portfolio was created after the internship to showcase the capabilities of the system, while the actual implementation at Hanon Systems was focused on HVAC component visualization and interaction.</p> <hr> <h3 id="7-skills--technologies-used">7. Skills &amp; Technologies Used</h3> <ul> <li> <strong>Languages &amp; Frameworks</strong>: Python, C++, PyTorch, ONNX, MediaPipe, WebSockets, OpenCV, NumPy</li> <li> <strong>Machine Learning</strong>: 3D Tracking, Landmark Detection, Depth Sensing, Geometric Analysis, Kalman Filtering</li> <li> <strong>Deep Learning</strong>: Recurrent Neural Networks (GRU), Model Optimization (ONNX), GPU Inference (Unity Barracuda)</li> <li> <strong>Computer Vision</strong>: Landmark Detection, Depth Filtering, Motion Analysis, Real-time Processing</li> <li> <strong>Augmented Reality</strong>: Unity Development, 3D Interaction Design, Virtual Object Manipulation</li> </ul> <hr> <h3 id="8-project-repository">8. Project Repository</h3> <ul> <li> <a href="https://github.com/Srecharan/VirtuHand.git" rel="external nofollow noopener" target="_blank">VirtuHand</a>: Main project repository</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Srecharan Selvam. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-repositories",title:"Repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-portfolio",title:"Portfolio",description:"A collection of my detailed technical work and projects",section:"Navigation",handler:()=>{window.location.href="/portfolio/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-",title:"",description:"",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-vision-language-action-enhanced-robotic-leaf-grasping",title:"Vision-Language-Action Enhanced Robotic Leaf Grasping",description:"A novel vision system combining computer vision, deep learning, and Vision-Language-Action models",section:"Projects",handler:()=>{window.location.href="/projects/1_project_minimal/"}},{id:"projects-deeptrade-ai-multi-model-stock-prediction-with-nlp-amp-automated-trading",title:"DeepTrade AI - Multi-Model Stock Prediction with NLP & Automated Trading",description:"An enterprise-grade system integrating LSTM-XGBoost prediction with sentiment analysis, distributed training infrastructure, and automated workflows",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-real-time-hand-gesture-recognition-for-ar-interaction",title:"Real-time Hand Gesture Recognition for AR Interaction",description:"A sophisticated system combining computer vision and deep learning for hand tracking and gesture recognition in augmented reality",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-multi-camera-vision-system-for-automated-material-detection-and-sorting",title:"Multi-Camera Vision System for Automated Material Detection and Sorting",description:"A real-time computer vision system for material recovery and worker safety monitoring on industrial conveyor belts",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-",title:"",description:"",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-genai-for-synthetic-data-augmentation",title:"GenAI for Synthetic Data Augmentation",description:"Validating synthetic data augmentation for computer vision with measurable accuracy improvements",section:"Projects",handler:()=>{window.location.href="/projects/5_project_minimal/"}},{id:"projects-high-fidelity-3d-scene-reconstruction-integrating-diffusion-models-with-memory-efficient-neural-radiance-fields",title:"High-Fidelity 3D Scene Reconstruction Integrating Diffusion Models with Memory-Efficient Neural Radiance Fields",description:"A novel approach combining diffusion models with Neural Radiance Fields for high-quality 3D scene reconstruction",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-safetyvlm-vlm-based-tool-recognition-system-for-industrial-safety-applications",title:"SafetyVLM: VLM-Based Tool Recognition System for Industrial Safety Applications",description:"A comprehensive system for tool recognition and safety guidance using fine-tuned vision-language models with LangChain RAG, Pinecone, Docker, and Kubernetes deployment",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%73%65%6C%76%61%6D@%61%6E%64%72%65%77.%63%6D%75.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=Srecharan Selvam","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Srecharan","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/srecharan","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>