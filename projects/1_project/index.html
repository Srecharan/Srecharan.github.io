<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h3 id="1-overview">1. Overview</h3> <p>A real-time vision system for leaf manipulation that combines geometric computer vision with deep learning and <strong>Vision-Language-Action (VLA) models</strong>. This hybrid system integrates YOLOv8 for segmentation, RAFT-Stereo for depth estimation, and a custom CNN enhanced with <strong>LLaVA-1.6-Mistral-7B for intelligent grasp reasoning</strong>. The architecture features <strong>self-supervised learning eliminating 100% manual annotation</strong>, <strong>LoRA fine-tuning achieving 88% validation accuracy</strong>, and a confidence-weighted framework dynamically balancing traditional CV, ML, and VLA predictions. Production optimizations include <strong>custom CUDA kernels</strong>, <strong>TensorRT acceleration</strong>, and <strong>Docker containerization</strong> with <strong>AWS GPU training infrastructure</strong>.</p> <div style="text-align: center;"> <img src="../assets/img/project-1/REX.drawiof.png" alt="System Architecture" style="width: 100%; max-width: 3000px;"> <p><em>Multi-stage perception pipeline enhanced with Vision-Language-Action integration</em></p> </div> <hr> <h3 id="2-vision-language-action-vla-system">2. Vision-Language-Action (VLA) System</h3> <h4 id="21-llava-integration-and-fine-tuning">2.1 LLaVA Integration and Fine-tuning</h4> <p><strong>Foundation Model Enhancement for Grasp Reasoning</strong></p> <p>Integrated LLaVA-1.6-Mistral-7B foundation model with parameter-efficient LoRA fine-tuning:</p> <ul> <li> <strong>Base Model</strong>: LLaVA-1.6-Mistral-7B (CLIP + Vicuna) for vision-language understanding</li> <li> <strong>Fine-tuning</strong>: LoRA adaptation (rank=8, alpha=32) for leaf grasping tasks</li> <li> <strong>Training Infrastructure</strong>: AWS GPU acceleration with MLflow experiment tracking</li> <li> <strong>Performance</strong>: 88.0% validation accuracy through systematic hyperparameter optimization</li> <li> <strong>Experiments</strong>: 4 systematic configurations (baseline_5e5, higher_lr_1e4, larger_rank_16, optimized_config)</li> </ul> <h4 id="22-hybrid-cv-vla-decision-framework">2.2 Hybrid CV-VLA Decision Framework</h4> <p><strong>Dynamic Confidence-Based Integration</strong></p> <p>The system implements intelligent fusion between traditional CV, ML, and VLA predictions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Confidence-based weighting strategy
</span><span class="k">if</span> <span class="n">vla_confidence</span> <span class="o">&gt;</span> <span class="mf">0.8</span><span class="p">:</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>  <span class="c1"># CV, ML, VLA
</span><span class="k">elif</span> <span class="n">vla_confidence</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># Conservative VLA influence
</span><span class="k">else</span><span class="p">:</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># Pure CV fallback
</span></code></pre></div></div> <ul> <li> <strong>High VLA Confidence</strong>: Balanced three-way integration</li> <li> <strong>Medium Confidence</strong>: CV-dominant with VLA assistance</li> <li> <strong>Low Confidence</strong>: Traditional CV fallback for reliability</li> <li> <strong>Adaptive Learning</strong>: VLA influence grows with operational experience</li> </ul> <hr> <h3 id="3-multi-stage-perception-pipeline">3. Multi-Stage Perception Pipeline</h3> <h4 id="31-instance-segmentation-yolov8">3.1 Instance Segmentation (YOLOv8)</h4> <p>Fine-tuned on ~900 images achieving 68% mAP@[0.5:0.95] with TensorRT optimization for real-time performance.</p> <h4 id="32-depth-estimation-raft-stereo">3.2 Depth Estimation (RAFT-Stereo)</h4> <p>High-precision depth maps with sub-pixel accuracy (&lt;0.5px) enhanced with custom CUDA kernels for accelerated point cloud generation.</p> <div style="text-align: center; display: flex; justify-content: center; gap: 10px; flex-wrap: wrap;"> <img src="/assets/img/project-1/rgb_input.png" alt="RGB Input" style="width: 30%; max-width: 300px;"> <img src="/assets/img/project-1/depth0.png" alt="Depth Map" style="width: 30%; max-width: 300px;"> <img src="/assets/img/project-1/plant_pcd-2x.gif" alt="Point Cloud" style="width: 30%; max-width: 300px;"> </div> <div style="text-align: center;"> <p><em>Stereo vision pipeline: RGB input → depth estimation → 3D reconstruction</em></p> </div> <h4 id="33-hybrid-grasp-point-selection">3.3 Hybrid Grasp Point Selection</h4> <h5 id="traditional-cv-pipeline">Traditional CV Pipeline</h5> <p>Pareto optimization for leaf selection with geometric scoring:</p> <ul> <li> <strong>Clutter Score (35%)</strong>: Isolation using Signed Distance Fields</li> <li> <strong>Distance Score (35%)</strong>: Camera proximity with exponential falloff</li> <li> <strong>Visibility Score (30%)</strong>: Frame position and completeness</li> </ul> <p>Grasp point selection criteria:</p> <ul> <li> <strong>Flatness Analysis (25%)</strong>: Surface smoothness via depth gradients</li> <li> <strong>Approach Vector Quality (40%)</strong>: Optimal robot orientation</li> <li> <strong>Accessibility (15%)</strong>: Camera-relative positioning</li> <li> <strong>Edge Awareness (20%)</strong>: Boundary distance analysis</li> </ul> <h5 id="ml-enhancement-with-mlflow-tracking">ML Enhancement with MLflow Tracking</h5> <p>Custom GraspPointCNN with comprehensive experiment management:</p> <ul> <li> <strong>Self-Supervised Learning</strong>: CV-generated training data (100% annotation-free)</li> <li> <strong>MLflow Integration</strong>: 60+ tracked experiments across attention mechanisms</li> <li> <strong>Architecture</strong>: 9-channel input with spatial/channel attention</li> <li> <strong>Performance</strong>: 93.14% validation accuracy, 94.79% F1 score</li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-1/CNN_grasp.drawio.png" alt="CNN Architecture" style="width: 50%; max-width: 400px;"> <p><em>GraspPointCNN with attention mechanism for grasp quality prediction</em></p> </div> <h5 id="vla-enhanced-decision-making">VLA-Enhanced Decision Making</h5> <p>Language-guided grasp reasoning with confidence weighting:</p> <ul> <li> <strong>Prompt Engineering</strong>: Structured queries for grasp point evaluation</li> <li> <strong>Confidence Scoring</strong>: Dynamic assessment of VLA prediction quality</li> <li> <strong>Fallback Strategy</strong>: Robust degradation to proven CV algorithms</li> <li> <strong>Continuous Learning</strong>: Adaptation through operational feedback</li> </ul> <div style="text-align: center; display: flex; justify-content: center; flex-wrap: wrap;"> <img src="/assets/img/project-1/cv_op1.png" alt="CV Output 1" style="max-width: 400px;"> <img src="/assets/img/project-1/cv_op2.png" alt="CV Output 2" style="max-width: 400px;"> </div> <div style="text-align: center;"> <p><em>Hybrid CV-ML-VLA pipeline: Traditional geometric analysis (left) enhanced with foundation model reasoning (right)</em></p> </div> <hr> <h3 id="4-production-optimization">4. Production Optimization</h3> <h4 id="41-custom-cuda-kernel-development">4.1 Custom CUDA Kernel Development</h4> <p>Developed GPU kernels addressing CPU bottlenecks in point cloud generation:</p> <ul> <li> <strong>Performance</strong>: 5x speedup (150ms → 30ms) for real-time operation</li> <li> <strong>Implementation</strong>: Parallelized 1.5M pixel processing with memory optimization</li> </ul> <h4 id="42-tensorrt--model-optimization">4.2 TensorRT &amp; Model Optimization</h4> <p>System-wide acceleration with model compilation:</p> <ul> <li> <strong>Models</strong>: YOLOv8, RAFT-Stereo, GraspPointCNN, and LLaVA components</li> <li> <strong>Techniques</strong>: FP16 precision, operator fusion, graph optimization</li> <li> <strong>Results</strong>: 35% throughput improvement (20 → 27 FPS)</li> </ul> <h4 id="43-aws-training-infrastructure">4.3 AWS Training Infrastructure</h4> <p>Cloud-based training pipeline for VLA fine-tuning:</p> <ul> <li> <strong>Infrastructure</strong>: g4dn.xlarge instances with Tesla T4 GPUs</li> <li> <strong>Cost Efficiency</strong>: LoRA fine-tuning reduces computational requirements</li> <li> <strong>Scalability</strong>: MLflow tracking across distributed experiments</li> <li> <strong>Deployment</strong>: Docker containerization for environment consistency</li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-1/rex_grasp_4x.gif" alt="System Operation" style="width: 30%; max-width: 240px;"> <p><em>Production-optimized VLA-enhanced grasping system in operation</em></p> </div> <hr> <h3 id="5-results--performance">5. Results &amp; Performance</h3> <h4 id="vla-system-performance">VLA System Performance</h4> <div class="table-responsive"> <table class="table"> <thead> <tr> <th>Model</th> <th>Configuration</th> <th>Validation Accuracy</th> <th>Training Infrastructure</th> </tr> </thead> <tbody> <tr> <td>LLaVA-1.6-Mistral-7B</td> <td>baseline_5e5</td> <td><strong>88.0%</strong></td> <td>AWS GPU + MLflow</td> </tr> <tr> <td>GraspPointCNN</td> <td>Spatial Attention</td> <td>93.14%</td> <td>Self-supervised</td> </tr> <tr> <td>Hybrid Integration</td> <td>Confidence-weighted</td> <td>82.66% field success</td> <td>Production deployment</td> </tr> </tbody> </table> </div> <h4 id="system-performance-comparison-150-test-cases">System Performance Comparison (150 test cases)</h4> <div class="table-responsive"> <table class="table"> <thead> <tr> <th>Metric</th> <th>Classical CV</th> <th>Hybrid (CV+ML+VLA)</th> <th>Improvement</th> </tr> </thead> <tbody> <tr> <td>Overall Success Rate (%)</td> <td>78.00</td> <td><strong>82.66</strong></td> <td>+4.66</td> </tr> <tr> <td>Feature Alignment (%)</td> <td>80.67</td> <td>83.33</td> <td>+2.66</td> </tr> <tr> <td>Edge Case Handling (%)</td> <td>75.33</td> <td>77.33</td> <td>+2.00</td> </tr> <tr> <td>Accuracy (px)</td> <td>25.3</td> <td>27.1</td> <td>+1.8</td> </tr> </tbody> </table> </div> <h4 id="production-optimization-results">Production Optimization Results</h4> <div class="table-responsive"> <table class="table"> <thead> <tr> <th>Component</th> <th>Baseline</th> <th>Optimized</th> <th>Improvement</th> </tr> </thead> <tbody> <tr> <td>VLA Training (AWS)</td> <td>CPU-only</td> <td>GPU acceleration</td> <td>3x speedup</td> </tr> <tr> <td>Point Cloud Generation</td> <td>150ms</td> <td>30ms</td> <td>5x speedup</td> </tr> <tr> <td>Inference Throughput</td> <td>20 FPS</td> <td>27 FPS</td> <td>35% improvement</td> </tr> <tr> <td>Dataset Creation</td> <td>Manual annotation</td> <td>Self-supervised</td> <td>100% elimination</td> </tr> </tbody> </table> </div> <hr> <h3 id="6-key-contributions">6. Key Contributions</h3> <p><strong>System Development:</strong></p> <ul> <li>Complete VLA integration pipeline with LLaVA-1.6-Mistral-7B foundation model</li> <li>LoRA fine-tuning achieving 88% validation accuracy through systematic optimization</li> <li>Hybrid decision framework balancing CV, ML, and VLA predictions with confidence weighting</li> <li>Self-supervised learning eliminating manual annotation requirements</li> <li>AWS GPU training infrastructure with MLflow experiment tracking</li> </ul> <p><strong>Performance Optimization:</strong></p> <ul> <li>Custom CUDA kernels for 5x point cloud generation speedup</li> <li>TensorRT model compilation for 35% inference improvement</li> <li>Production Docker deployment achieving 82.66% field success rate</li> </ul> <p>This research is conducted under Prof. Abhisesh Silwal and Prof. George A. Kantor.</p> <hr> <h3 id="7-skills-and-technologies">7. Skills and Technologies</h3> <ul> <li> <strong>Foundation Models</strong>: LLaVA-1.6-Mistral-7B, LoRA Fine-tuning, Vision-Language Integration</li> <li> <strong>Languages</strong>: Python, C++, CUDA</li> <li> <strong>Deep Learning</strong>: PyTorch, CNN Architecture, Self-Supervised Learning, Attention Mechanisms</li> <li> <strong>Computer Vision</strong>: Instance Segmentation, Depth Estimation, Point Cloud Processing, 3D Perception</li> <li> <strong>MLOps</strong>: MLflow Experiment Tracking, Model Versioning, Hyperparameter Optimization</li> <li> <strong>Cloud &amp; Performance</strong>: AWS EC2/GPU, Custom CUDA Kernels, TensorRT Optimization</li> <li> <strong>Production</strong>: Docker Containerization, ROS2 Integration, Real-time Systems</li> </ul> <hr> <h3 id="8-project-repositories">8. Project Repositories</h3> <ul> <li> <a href="https://github.com/Srecharan/Leaf-Grasping-Vision-ML.git" rel="external nofollow noopener" target="_blank">LeafGrasp-Vision-ML</a>: <strong>Main Repository with VLA System Integration</strong> </li> <li> <a href="https://github.com/Srecharan/YoloV8Seg-REX.git" rel="external nofollow noopener" target="_blank">YOLOv8 Segmentation</a>: Real-time Leaf Instance Segmentation</li> <li> <a href="https://github.com/Srecharan/RAFTStereo-REX.git" rel="external nofollow noopener" target="_blank">RAFT-Stereo</a>: High-Precision Depth Estimation with CUDA</li> <li> <a href="https://github.com/Srecharan/REX-Robot.git" rel="external nofollow noopener" target="_blank">REX-Robot</a>: 6-DOF Gantry Robot Control System</li> </ul> <hr> <h3 id="9-references">9. References</h3> <p>[1] Srecharan Selvam, Abhisesh Silwal, George Kantor “Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach”, https://arxiv.org/pdf/2505.0370, Under review at ICCV 2025.</p> <p>[2] Silwal, A., Zhang, X. M., Hadlock, T., Neice, J., Haque, S., Kaundanya, A., Lu, C., Vinatzer, B. A., Kantor, G., &amp; Li, S. (2024). Towards an AI-Driven Cyber-Physical System for Closed-Loop Control of Plant Diseases. <em>Proceedings of the AAAI Symposium Series</em>, <em>4</em>(1), 432-435.</p> </body></html>