<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Hybrid CV-ML Approach for Autonomous Leaf Grasping | Srecharan Selvam </title> <meta name="author" content="Srecharan Selvam"> <meta name="description" content="A novel vision system combining geometric computer vision with deep learning for leaf detection and grasp point optimization"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon2.png?f718b847644d9f9675c29095ed121769"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://srecharan.github.io/projects/1_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Srecharan Selvam </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/portfolio/">Portfolio </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Hybrid CV-ML Approach for Autonomous Leaf Grasping</h1> <p class="post-description">A novel vision system combining geometric computer vision with deep learning for leaf detection and grasp point optimization</p> </header> <article> <h3 id="1-overview">1. Overview</h3> <p>A real-time vision system for leaf manipulation combining geometric computer vision techniques with deep learning. This hybrid system integrates YOLOv8 for leaf segmentation, RAFT-Stereo for depth estimation, and a custom CNN (GraspPointCNN) for grasp point optimization. The architecture features self-supervised learning that eliminates manual annotation, and a confidence-weighted decision framework that dynamically balances traditional CV algorithms with CNN predictions to achieve superior grasping performance.</p> <div style="text-align: center;"> <img src="/assets/img/project-1/REX.drawio_f.png" alt="System Architecture" style="width: 100%; max-width: 3000px;"> <p><em>Multi-stage perception pipeline for leaf manipulation</em></p> </div> <hr> <h3 id="2-multi-stage-perception-pipeline">2. Multi-Stage Perception Pipeline</h3> <p>The system employs a three-stage perception pipeline:</p> <h4 id="21-instance-segmentation-yolov8">2.1 Instance Segmentation (YOLOv8)</h4> <p>Fine-tuned on a custom dataset of ~900 images, achieving 68% mAP@[0.5:0.95] for leaf mask generation</p> <div style="text-align: center;"> <img src="/assets/img/project-1/yolo_output.png" alt="YOLOv8 Segmentation Output" style="width: 100%; max-width: 800px;"> <p><em>YOLOv8 instance segmentation results showing precise leaf mask generation with high confidence scores</em></p> </div> <h4 id="22-depth-estimation-raft-stereo">2.2 Depth Estimation (RAFT-Stereo)</h4> <p>High-precision depth maps with sub-pixel accuracy (&lt;0.5px) from stereo pairs, enabling detailed 3D reconstruction</p> <div style="text-align: center; display: flex; justify-content: center; gap: 10px; flex-wrap: wrap;"> <img src="/assets/img/project-1/rgb_input.png" alt="RGB Input" style="width: 30%; max-width: 300px;"> <img src="/assets/img/project-1/depth0.png" alt="Depth Map" style="width: 30%; max-width: 300px;"> <img src="/assets/img/project-1/plant_pcd-2x.gif" alt="Point Cloud" style="width: 30%; max-width: 300px;"> </div> <div style="text-align: center;"> <p><em>Stereo vision pipeline: RGB input (left), depth map visualization (center), and 3D point cloud reconstruction (right)</em></p> </div> <h4 id="23-hybrid-grasp-point-selection">2.3 Hybrid Grasp Point Selection</h4> <p>Combines geometric CV with machine learning refinement, which includes the traditional CV pipeline and ML enhancement described below.</p> <h5 id="231-traditional-computer-vision-pipeline">2.3.1 Traditional Computer Vision Pipeline</h5> <p>The geometric CV component uses Pareto optimization for leaf selection based on:</p> <ul> <li> <strong>Clutter Score (35%)</strong>: Isolation from other leaves using Signed Distance Fields</li> <li> <strong>Distance Score (35%)</strong>: Proximity to camera with exponential falloff</li> <li> <strong>Visibility Score (30%)</strong>: Completeness of view and position in frame</li> </ul> <p>Grasp point selection employs weighted scoring criteria:</p> <ul> <li> <strong>Flatness Analysis (25%)</strong>: Surface smoothness using depth gradients</li> <li> <strong>Approach Vector Quality (40%)</strong>: Optimal approach direction</li> <li> <strong>Accessibility (15%)</strong>: Position relative to camera</li> <li> <strong>Edge Awareness (20%)</strong>: Distance from leaf boundaries</li> </ul> <div style="text-align: center; display: flex; justify-content: center; flex-wrap: wrap;"> <img src="/assets/img/project-1/cv_op1.png" alt="CV Output 1" style="max-width: 400px;"> <img src="/assets/img/project-1/cv_op2.png" alt="CV Output 2" style="max-width: 400px;"> </div> <div style="text-align: center;"> <p><em>Traditional CV pipeline output: Raw stereo camera image with detected leaf midrib (left); Segmented leaf visualization with grasp point selection (right)</em></p> </div> <h5 id="232-ml-enhanced-decision-making">2.3.2 ML-Enhanced Decision Making</h5> <p>The machine learning component features a custom CNN architecture (GraspPointCNN) with:</p> <ul> <li> <strong>Self-Supervised Learning</strong>: CV pipeline acts as an expert teacher</li> <li> <strong>Data Collection</strong>: Automated generation of positive/negative samples</li> <li> <strong>9-Channel Input Features</strong>: Depth map, binary mask, and 7 score maps</li> <li> <strong>Attention Mechanism</strong>: Enables focus on most relevant patch regions</li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-1/CNN_grasp.drawio.png" alt="CNN Architecture" style="width: 50%; max-width: 400px;"> <p><em>GraspPointCNN architecture</em></p> </div> <h5 id="233-hybrid-decision-integration">2.3.3 Hybrid Decision Integration</h5> <p>The system implements a dynamic integration strategy:</p> <ul> <li>Traditional CV generates candidate grasp points</li> <li>ML model evaluates candidates with confidence scores</li> <li>Weighted average combines both approaches: <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">ml_conf</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="nf">abs</span><span class="p">(</span><span class="n">ml_score</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">ml_weight</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">ml_conf</span> <span class="o">*</span> <span class="mf">0.6</span><span class="p">)</span>
<span class="n">final_score</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">ml_weight</span><span class="p">)</span> <span class="o">*</span> <span class="n">trad_score</span> <span class="o">+</span> <span class="n">ml_weight</span> <span class="o">*</span> <span class="n">ml_score</span>
</code></pre></div> </div> </li> <li>ML influence varies (10-30%) based on prediction confidence</li> <li>Falls back to traditional CV (70-90%) for low-confidence predictions</li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-1/hybrid_op.png" alt="Hybrid Output" style="width: 100%; max-width: 800px;"> <p><em>Hybrid CV-ML grasp point selection: Left - Original camera view with leaf midrib ; Right - Segmented leaves with grasp point visualization </em></p> </div> <p style="font-style: italic; font-weight: underline;"> <span style="font-weight: bold; font-style: italic;">Note:</span> Grasping at the leaf tip often fails as the REX robot struggles to secure it, leading to missed grasps or leaf displacement. The hybrid grasp point selection method outperforms traditional CV, achieving a 4.66% improvement over 150 test cases. </p> <div style="text-align: center;"> <img src="/assets/img/project-1/rex_grasp_4x.gif" alt="System Operation" style="width: 30%; max-width: 240px;"> <p><em>Complete pipeline in action: Once 3D coordinates are determined, the system executes precise leaf grasping</em></p> </div> <hr> <h3 id="3-results--performance">3. Results &amp; Performance</h3> <h4 id="model-metrics">Model Metrics</h4> <div class="table-responsive"> <table class="table"> <thead> <tr> <th>Metric</th> <th>Value</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td>Validation Accuracy</td> <td>93.14%</td> <td>Overall model accuracy</td> </tr> <tr> <td>Positive Accuracy</td> <td>97.09%</td> <td>Accuracy for successful grasp points</td> </tr> <tr> <td>Precision</td> <td>92.59%</td> <td>True positives / predicted positives</td> </tr> <tr> <td>Recall</td> <td>97.09%</td> <td>True positives / actual positives</td> </tr> <tr> <td>F1 Score</td> <td>94.79%</td> <td>Balanced measure of precision and recall</td> </tr> </tbody> </table> </div> <h4 id="system-performance-150-test-cases">System Performance (150 test cases)</h4> <div class="table-responsive"> <table class="table"> <thead> <tr> <th>Metric</th> <th>Classical CV</th> <th>Hybrid (CV+ML)</th> <th>Improvement</th> </tr> </thead> <tbody> <tr> <td>Accuracy (px)</td> <td>25.3</td> <td>27.1</td> <td>+1.8</td> </tr> <tr> <td>Feature Alignment (%)</td> <td>80.67</td> <td>83.33</td> <td>+2.66</td> </tr> <tr> <td>Edge Case Handling (%)</td> <td>75.33</td> <td>77.33</td> <td>+2.00</td> </tr> <tr> <td>Overall Success Rate (%)</td> <td>78.00</td> <td>82.66</td> <td>+4.66</td> </tr> </tbody> </table> </div> <hr> <h3 id="4-contribution">4. Contribution</h3> <p>I was responsible for the complete system development including:</p> <ul> <li>Preparation and annotation of custom dataset for YOLOv8 instance segmentation, along with training and validating the model</li> <li>Fine-tuning the RAFT-Stereo model for high-precision depth estimation and 3D reconstruction</li> <li>Engineered the traditional computer vision pipeline with multiple scoring mechanisms, optimal leaf selection, and grasping point detection</li> <li>Designed and implemented the custom CNN architecture (GraspPointCNN)</li> <li>Created the hybrid decision integration framework that balances traditional CV with ML refinement</li> <li>Testing and validating system performance through multiple experimental trials</li> </ul> <p>This research is carried out under the guidance of Prof. Abhisesh Silwal and Prof. George A. Kantor.</p> <hr> <h3 id="5-skills-and-technologies-used">5. Skills and Technologies Used</h3> <ul> <li> <strong>Languages</strong>: Python, C++</li> <li> <strong>Frameworks</strong>: PyTorch, CUDA, OpenCV, Scikit-learn, Numpy, Pandas, Matplotlib, ROS2</li> <li> <strong>Computer Vision</strong>: Instance Segmentation, Depth Estimation, Point Cloud Processing, SDF, 3D Perception</li> <li> <strong>Deep Learning</strong>: CNN Architecture Design, Self-Supervised Learning, Model Training &amp; Optimization, Attention Mechanisms</li> <li> <strong>Cloud Computing</strong>: AWS EC2</li> </ul> <hr> <h3 id="6-project-repositories">6. Project Repositories</h3> <ul> <li> <a href="https://github.com/Srecharan/Leaf-Grasping-Vision-ML.git" rel="external nofollow noopener" target="_blank">LeafGrasp-Vision-ML</a>: Main Project Repository</li> <li><a href="https://github.com/Srecharan/YoloV8Seg-REX.git" rel="external nofollow noopener" target="_blank">YOLOv8 Segmentation Node</a></li> <li><a href="https://github.com/Srecharan/RAFTStereo-REX.git" rel="external nofollow noopener" target="_blank">RAFT-Stereo Node</a></li> <li><a href="https://github.com/Srecharan/REX-Robot.git" rel="external nofollow noopener" target="_blank">REX-Robot</a></li> </ul> <hr> <h3 id="7-references">7. References</h3> <p>[1] Silwal, A., Zhang, X. M., Hadlock, T., Neice, J., Haque, S., Kaundanya, A., Lu, C., Vinatzer, B. A., Kantor, G., &amp; Li, S. (2024). Towards an AI-Driven Cyber-Physical System for Closed-Loop Control of Plant Diseases. <em>Proceedings of the AAAI Symposium Series</em>, <em>4</em>(1), 432-435. https://doi.org/10.1609/aaaiss.v4i1.31828</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Srecharan Selvam. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-repositories",title:"Repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-portfolio",title:"Portfolio",description:"A collection of my detailed technical work and projects",section:"Navigation",handler:()=>{window.location.href="/portfolio/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-hybrid-cv-ml-approach-for-autonomous-leaf-grasping",title:"Hybrid CV-ML Approach for Autonomous Leaf Grasping",description:"A novel vision system combining geometric computer vision with deep learning for leaf detection and grasp point optimization",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-deeptrade-ai-multi-model-stock-prediction-with-nlp-amp-automated-trading",title:"DeepTrade AI - Multi-Model Stock Prediction with NLP & Automated Trading",description:"An end-to-end system integrating LSTM-XGBoost prediction with sentiment analysis for automated trading",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-real-time-hand-gesture-recognition-for-ar-interaction",title:"Real-time Hand Gesture Recognition for AR Interaction",description:"A sophisticated system combining computer vision and deep learning for hand tracking and gesture recognition in augmented reality",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-multi-camera-vision-system-for-automated-material-detection-and-sorting",title:"Multi-Camera Vision System for Automated Material Detection and Sorting",description:"A real-time computer vision system for material recovery and worker safety monitoring on industrial conveyor belts",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-deep-image-synthesis-with-gans-vaes-and-diffusion-models",title:"Deep Image Synthesis with GANs, VAEs, and Diffusion Models",description:"A comprehensive implementation of three leading generative model architectures for image synthesis",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-high-fidelity-3d-scene-reconstruction-integrating-diffusion-models-with-memory-efficient-neural-radiance-fields",title:"High-Fidelity 3D Scene Reconstruction Integrating Diffusion Models with Memory-Efficient Neural Radiance Fields",description:"A novel approach combining diffusion models with Neural Radiance Fields for high-quality 3D scene reconstruction",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-safetyvlm-vlm-based-tool-recognition-system-for-industrial-safety-applications",title:"SafetyVLM: VLM-Based Tool Recognition System for Industrial Safety Applications",description:"A comprehensive system for tool recognition and safety guidance using fine-tuned vision-language models with LangChain RAG, Pinecone, Docker, and Kubernetes deployment",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%73%65%6C%76%61%6D@%61%6E%64%72%65%77.%63%6D%75.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=Srecharan Selvam","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Srecharan","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/srecharan","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>