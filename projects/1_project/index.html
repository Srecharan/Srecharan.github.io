<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Vision-Language-Action Enhanced Robotic Leaf Manipulation | Srecharan Selvam </title> <meta name="author" content="Srecharan Selvam"> <meta name="description" content="A novel vision system combining geometric computer vision, deep learning, and Vision-Language-Action models for intelligent leaf manipulation"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon2.png?f718b847644d9f9675c29095ed121769"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://srecharan.github.io/projects/1_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Srecharan Selvam </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/portfolio/">Portfolio </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Vision-Language-Action Enhanced Robotic Leaf Manipulation</h1> <p class="post-description">A novel vision system combining geometric computer vision, deep learning, and Vision-Language-Action models for intelligent leaf manipulation</p> </header> <article> <h2 id="overview">Overview</h2> <p>A real-time vision system for leaf manipulation that combines geometric computer vision with deep learning and <strong>Vision-Language-Action (VLA) models</strong>. This hybrid system integrates YOLOv8 for segmentation, RAFT-Stereo for depth estimation, and a custom CNN enhanced with <strong>LLaVA-1.6-Mistral-7B for intelligent grasp reasoning</strong>.</p> <p>Key achievements:</p> <ul> <li><strong>Self-supervised learning eliminating 100% manual annotation</strong></li> <li><strong>LoRA fine-tuning achieving 88% validation accuracy</strong></li> <li> <strong>Confidence-weighted framework</strong> dynamically balancing traditional CV, ML, and VLA predictions</li> <li> <strong>Custom CUDA kernels</strong> and <strong>TensorRT acceleration</strong> </li> <li> <strong>AWS GPU training infrastructure</strong> with <strong>Docker containerization</strong> </li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project-1/REX.drawiof-480.webp 480w,/assets/img/project-1/REX.drawiof-800.webp 800w,/assets/img/project-1/REX.drawiof-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project-1/REX.drawiof.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="System Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Multi-stage perception pipeline enhanced with Vision-Language-Action integration </div> <hr> <h2 id="vision-language-action-vla-system">Vision-Language-Action (VLA) System</h2> <h3 id="llava-integration-and-fine-tuning">LLaVA Integration and Fine-tuning</h3> <p><strong>Foundation Model Enhancement for Grasp Reasoning</strong></p> <p>Integrated LLaVA-1.6-Mistral-7B foundation model with parameter-efficient LoRA fine-tuning:</p> <ul> <li> <strong>Base Model</strong>: LLaVA-1.6-Mistral-7B (CLIP + Vicuna) for vision-language understanding</li> <li> <strong>Fine-tuning</strong>: LoRA adaptation (rank=8, alpha=32) for leaf grasping tasks</li> <li> <strong>Training Infrastructure</strong>: AWS GPU acceleration with MLflow experiment tracking</li> <li> <strong>Performance</strong>: 88.0% validation accuracy through systematic hyperparameter optimization</li> <li> <strong>Experiments</strong>: 4 systematic configurations with comprehensive evaluation</li> </ul> <h3 id="hybrid-cv-vla-decision-framework">Hybrid CV-VLA Decision Framework</h3> <p><strong>Dynamic Confidence-Based Integration</strong></p> <p>The system implements intelligent fusion between traditional CV, ML, and VLA predictions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Confidence-based weighting strategy
</span><span class="k">if</span> <span class="n">vla_confidence</span> <span class="o">&gt;</span> <span class="mf">0.8</span><span class="p">:</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)</span>  <span class="c1"># CV, ML, VLA
</span><span class="k">elif</span> <span class="n">vla_confidence</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># Conservative VLA influence
</span><span class="k">else</span><span class="p">:</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># Pure CV fallback
</span></code></pre></div></div> <ul> <li> <strong>High VLA Confidence</strong>: Balanced three-way integration</li> <li> <strong>Medium Confidence</strong>: CV-dominant with VLA assistance</li> <li> <strong>Low Confidence</strong>: Traditional CV fallback for reliability</li> <li> <strong>Adaptive Learning</strong>: VLA influence grows with operational experience</li> </ul> <hr> <h2 id="multi-stage-perception-pipeline">Multi-Stage Perception Pipeline</h2> <h3 id="instance-segmentation-yolov8">Instance Segmentation (YOLOv8)</h3> <p>Fine-tuned on approximately 900 images achieving 68% mAP@[0.5:0.95] with TensorRT optimization for real-time performance.</p> <h3 id="depth-estimation-raft-stereo">Depth Estimation (RAFT-Stereo)</h3> <p>High-precision depth maps with sub-pixel accuracy (&lt;0.5px) enhanced with custom CUDA kernels for accelerated point cloud generation.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project-1/rgb_input-480.webp 480w,/assets/img/project-1/rgb_input-800.webp 800w,/assets/img/project-1/rgb_input-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project-1/rgb_input.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="RGB Input" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project-1/depth0-480.webp 480w,/assets/img/project-1/depth0-800.webp 800w,/assets/img/project-1/depth0-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project-1/depth0.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Depth Map" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project-1/plant_pcd-2x-480.webp 480w,/assets/img/project-1/plant_pcd-2x-800.webp 800w,/assets/img/project-1/plant_pcd-2x-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project-1/plant_pcd-2x.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Point Cloud" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Stereo vision pipeline: RGB input → depth estimation → 3D reconstruction </div> <h3 id="hybrid-grasp-point-selection">Hybrid Grasp Point Selection</h3> <h4 id="traditional-cv-pipeline">Traditional CV Pipeline</h4> <p>Pareto optimization for leaf selection with geometric scoring:</p> <ul> <li> <strong>Clutter Score (35%)</strong>: Isolation using Signed Distance Fields</li> <li> <strong>Distance Score (35%)</strong>: Camera proximity with exponential falloff</li> <li> <strong>Visibility Score (30%)</strong>: Frame position and completeness</li> </ul> <p>Grasp point selection criteria:</p> <ul> <li> <strong>Flatness Analysis (25%)</strong>: Surface smoothness via depth gradients</li> <li> <strong>Approach Vector Quality (40%)</strong>: Optimal robot orientation</li> <li> <strong>Accessibility (15%)</strong>: Camera-relative positioning</li> <li> <strong>Edge Awareness (20%)</strong>: Boundary distance analysis</li> </ul> <h4 id="ml-enhancement-with-mlflow-tracking">ML Enhancement with MLflow Tracking</h4> <p>Custom GraspPointCNN with comprehensive experiment management:</p> <ul> <li> <strong>Self-Supervised Learning</strong>: CV-generated training data (100% annotation-free)</li> <li> <strong>MLflow Integration</strong>: 60+ tracked experiments across attention mechanisms</li> <li> <strong>Architecture</strong>: 9-channel input with spatial/channel attention</li> <li> <strong>Performance</strong>: 93.14% validation accuracy, 94.79% F1 score</li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project-1/CNN_grasp.drawio-480.webp 480w,/assets/img/project-1/CNN_grasp.drawio-800.webp 800w,/assets/img/project-1/CNN_grasp.drawio-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project-1/CNN_grasp.drawio.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="CNN Architecture" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> GraspPointCNN with attention mechanism for grasp quality prediction </div> <h4 id="vla-enhanced-decision-making">VLA-Enhanced Decision Making</h4> <p>Language-guided grasp reasoning with confidence weighting:</p> <ul> <li> <strong>Prompt Engineering</strong>: Structured queries for grasp point evaluation</li> <li> <strong>Confidence Scoring</strong>: Dynamic assessment of VLA prediction quality</li> <li> <strong>Fallback Strategy</strong>: Robust degradation to proven CV algorithms</li> <li> <strong>Continuous Learning</strong>: Adaptation through operational feedback</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project-1/cv_op1-480.webp 480w,/assets/img/project-1/cv_op1-800.webp 800w,/assets/img/project-1/cv_op1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project-1/cv_op1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="CV Output 1" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project-1/cv_op2-480.webp 480w,/assets/img/project-1/cv_op2-800.webp 800w,/assets/img/project-1/cv_op2-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project-1/cv_op2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="CV Output 2" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Hybrid CV-ML-VLA pipeline: Traditional geometric analysis (left) enhanced with foundation model reasoning (right) </div> <hr> <h2 id="production-optimization">Production Optimization</h2> <h3 id="custom-cuda-kernel-development">Custom CUDA Kernel Development</h3> <p>Developed GPU kernels addressing CPU bottlenecks in point cloud generation:</p> <ul> <li> <strong>Performance</strong>: 5x speedup (150ms → 30ms) for real-time operation</li> <li> <strong>Implementation</strong>: Parallelized 1.5M pixel processing with memory optimization</li> </ul> <h3 id="tensorrt--model-optimization">TensorRT &amp; Model Optimization</h3> <p>System-wide acceleration with model compilation:</p> <ul> <li> <strong>Models</strong>: YOLOv8, RAFT-Stereo, GraspPointCNN, and LLaVA components</li> <li> <strong>Techniques</strong>: FP16 precision, operator fusion, graph optimization</li> <li> <strong>Results</strong>: 35% throughput improvement (20 → 27 FPS)</li> </ul> <h3 id="aws-training-infrastructure">AWS Training Infrastructure</h3> <p>Cloud-based training pipeline for VLA fine-tuning:</p> <ul> <li> <strong>Infrastructure</strong>: g4dn.xlarge instances with Tesla T4 GPUs</li> <li> <strong>Cost Efficiency</strong>: LoRA fine-tuning reduces computational requirements</li> <li> <strong>Scalability</strong>: MLflow tracking across distributed experiments</li> <li> <strong>Deployment</strong>: Docker containerization for environment consistency</li> </ul> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/project-1/rex_grasp_4x-480.webp 480w,/assets/img/project-1/rex_grasp_4x-800.webp 800w,/assets/img/project-1/rex_grasp_4x-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/project-1/rex_grasp_4x.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="System Operation" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Production-optimized VLA-enhanced grasping system in operation </div> <hr> <h2 id="results--performance">Results &amp; Performance</h2> <h3 id="performance-results">Performance Results</h3> <h4 id="vla-system-performance">VLA System Performance</h4> <p><strong>Model Performance Metrics:</strong></p> <ul> <li> <strong>LLaVA-1.6-Mistral-7B</strong>: 88.0% validation accuracy with LoRA fine-tuning (AWS GPU + MLflow)</li> <li> <strong>GraspPointCNN</strong>: 93.14% validation accuracy with spatial attention (self-supervised)</li> <li> <strong>Hybrid Integration</strong>: 82.66% field success rate with confidence weighting (production deployment)</li> </ul> <h4 id="system-performance-comparison-150-test-cases">System Performance Comparison (150 test cases)</h4> <p><strong>Performance Improvements:</strong></p> <ul> <li> <strong>Overall Success Rate</strong>: 78.00% → <strong>82.66%</strong> (+4.66% improvement)</li> <li> <strong>Feature Alignment</strong>: 80.67% → 83.33% (+2.66% improvement)</li> <li> <strong>Edge Case Handling</strong>: 75.33% → 77.33% (+2.00% improvement)</li> <li> <strong>Accuracy</strong>: 25.3px → 27.1px (+1.8px improvement)</li> </ul> <h4 id="production-optimization-results">Production Optimization Results</h4> <p><strong>System Optimizations:</strong></p> <ul> <li> <strong>VLA Training (AWS)</strong>: CPU-only → GPU acceleration (3x speedup)</li> <li> <strong>Point Cloud Generation</strong>: 150ms → 30ms (5x speedup)</li> <li> <strong>Inference Throughput</strong>: 20 FPS → 27 FPS (35% improvement)</li> <li> <strong>Dataset Creation</strong>: Manual annotation → Self-supervised (100% elimination)</li> </ul> <hr> <h2 id="key-contributions">Key Contributions</h2> <p><strong>System Development:</strong></p> <ul> <li>Complete VLA integration pipeline with LLaVA-1.6-Mistral-7B foundation model</li> <li>LoRA fine-tuning achieving 88% validation accuracy through systematic optimization</li> <li>Hybrid decision framework balancing CV, ML, and VLA predictions with confidence weighting</li> <li>Self-supervised learning eliminating manual annotation requirements</li> <li>AWS GPU training infrastructure with MLflow experiment tracking</li> </ul> <p><strong>Performance Optimization:</strong></p> <ul> <li>Custom CUDA kernels for 5x point cloud generation speedup</li> <li>TensorRT model compilation for 35% inference improvement</li> <li>Production Docker deployment achieving 82.66% field success rate</li> </ul> <p>This research is conducted under Prof. Abhisesh Silwal and Prof. George A. Kantor.</p> <hr> <h2 id="skills-and-technologies">Skills and Technologies</h2> <ul> <li> <strong>Foundation Models</strong>: LLaVA-1.6-Mistral-7B, LoRA Fine-tuning, Vision-Language Integration</li> <li> <strong>Languages</strong>: Python, C++, CUDA</li> <li> <strong>Deep Learning</strong>: PyTorch, CNN Architecture, Self-Supervised Learning, Attention Mechanisms</li> <li> <strong>Computer Vision</strong>: Instance Segmentation, Depth Estimation, Point Cloud Processing, 3D Perception</li> <li> <strong>MLOps</strong>: MLflow Experiment Tracking, Model Versioning, Hyperparameter Optimization</li> <li> <strong>Cloud &amp; Performance</strong>: AWS EC2/GPU, Custom CUDA Kernels, TensorRT Optimization</li> <li> <strong>Production</strong>: Docker Containerization, ROS2 Integration, Real-time Systems</li> </ul> <hr> <h2 id="project-repositories">Project Repositories</h2> <ul> <li> <a href="https://github.com/Srecharan/Leaf-Grasping-Vision-ML.git" rel="external nofollow noopener" target="_blank">LeafGrasp-Vision-ML</a>: <strong>Main Repository with VLA System Integration</strong> </li> <li> <a href="https://github.com/Srecharan/YoloV8Seg-REX.git" rel="external nofollow noopener" target="_blank">YOLOv8 Segmentation</a>: Real-time Leaf Instance Segmentation</li> <li> <a href="https://github.com/Srecharan/RAFTStereo-REX.git" rel="external nofollow noopener" target="_blank">RAFT-Stereo</a>: High-Precision Depth Estimation with CUDA</li> <li> <a href="https://github.com/Srecharan/REX-Robot.git" rel="external nofollow noopener" target="_blank">REX-Robot</a>: 6-DOF Gantry Robot Control System</li> </ul> <hr> <h2 id="references">References</h2> <p>[1] Srecharan Selvam, Abhisesh Silwal, George Kantor “Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach”, https://arxiv.org/pdf/2505.0370, Under review at ICCV 2025.</p> <p>[2] Silwal, A., Zhang, X. M., Hadlock, T., Neice, J., Haque, S., Kaundanya, A., Lu, C., Vinatzer, B. A., Kantor, G., &amp; Li, S. (2024). Towards an AI-Driven Cyber-Physical System for Closed-Loop Control of Plant Diseases. <em>Proceedings of the AAAI Symposium Series</em>, <em>4</em>(1), 432-435.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Srecharan Selvam. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-repositories",title:"Repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-portfolio",title:"Portfolio",description:"A collection of my detailed technical work and projects",section:"Navigation",handler:()=>{window.location.href="/portfolio/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-vision-language-action-enhanced-robotic-leaf-manipulation",title:"Vision-Language-Action Enhanced Robotic Leaf Manipulation",description:"A novel vision system combining geometric computer vision, deep learning, and Vision-Language-Action models for intelligent leaf manipulation",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-deeptrade-ai-multi-model-stock-prediction-with-nlp-amp-automated-trading",title:"DeepTrade AI - Multi-Model Stock Prediction with NLP & Automated Trading",description:"An enterprise-grade system integrating LSTM-XGBoost prediction with sentiment analysis, distributed training infrastructure, and automated workflows",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-real-time-hand-gesture-recognition-for-ar-interaction",title:"Real-time Hand Gesture Recognition for AR Interaction",description:"A sophisticated system combining computer vision and deep learning for hand tracking and gesture recognition in augmented reality",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-multi-camera-vision-system-for-automated-material-detection-and-sorting",title:"Multi-Camera Vision System for Automated Material Detection and Sorting",description:"A real-time computer vision system for material recovery and worker safety monitoring on industrial conveyor belts",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-",title:"",description:"",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-genai-for-synthetic-data-augmentation",title:"GenAI for Synthetic Data Augmentation",description:"Validating synthetic data augmentation for computer vision with measurable accuracy improvements",section:"Projects",handler:()=>{window.location.href="/projects/5_project_minimal/"}},{id:"projects-high-fidelity-3d-scene-reconstruction-integrating-diffusion-models-with-memory-efficient-neural-radiance-fields",title:"High-Fidelity 3D Scene Reconstruction Integrating Diffusion Models with Memory-Efficient Neural Radiance Fields",description:"A novel approach combining diffusion models with Neural Radiance Fields for high-quality 3D scene reconstruction",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-safetyvlm-vlm-based-tool-recognition-system-for-industrial-safety-applications",title:"SafetyVLM: VLM-Based Tool Recognition System for Industrial Safety Applications",description:"A comprehensive system for tool recognition and safety guidance using fine-tuned vision-language models with LangChain RAG, Pinecone, Docker, and Kubernetes deployment",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%73%65%6C%76%61%6D@%61%6E%64%72%65%77.%63%6D%75.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=Srecharan Selvam","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Srecharan","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/srecharan","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>