<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> High-Fidelity 3D Scene Reconstruction Integrating Diffusion Models with Memory-Efficient Neural Radiance Fields | Srecharan Selvam </title> <meta name="author" content="Srecharan Selvam"> <meta name="description" content="A novel approach combining diffusion models with Neural Radiance Fields for high-quality 3D scene reconstruction"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon2.png?f718b847644d9f9675c29095ed121769"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://srecharan.github.io/projects/6_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Srecharan Selvam </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/portfolio/">Portfolio </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">High-Fidelity 3D Scene Reconstruction Integrating Diffusion Models with Memory-Efficient Neural Radiance Fields</h1> <p class="post-description">A novel approach combining diffusion models with Neural Radiance Fields for high-quality 3D scene reconstruction</p> </header> <article> <h3 id="1-overview">1. Overview</h3> <p>DiffusionNeRF-3D introduces a novel approach to 3D scene reconstruction by combining diffusion models with Neural Radiance Fields (NeRF). The project implements a two-stage pipeline that first uses a diffusion model to refine depth maps from RGBD images, which are then used by a memory-efficient NeRF model for high-quality 3D scene reconstruction. This integration addresses key limitations in traditional NeRF approaches, particularly for scenes with complex geometry or limited view coverage. The system was developed and evaluated using the NYU Depth V2 dataset, which provides RGB-D images of diverse indoor scenes.</p> <div style="text-align: center;"> <img src="/assets/img/project-6/overview.png" alt="RGB and Depth Map" style="width: 90%; max-width: 800px;"> <p><em>Example from NYU Depth V2 dataset showing RGB image (left) and corresponding depth map (right) used for training and evaluation</em></p> </div> <hr> <h3 id="2-technical-approach">2. Technical Approach</h3> <h4 id="21-diffusion-model-for-depth-refinement">2.1 Diffusion Model for Depth Refinement</h4> <p>The first stage of the pipeline uses a specialized diffusion model to enhance the quality of depth maps:</p> <ul> <li> <strong>UNet Architecture</strong>: Implemented with multi-scale feature processing, attention mechanisms at bottleneck, and feature enhancement blocks</li> <li> <strong>Training Strategy</strong>: Cosine noise scheduling with advanced edge-aware loss functions</li> <li> <strong>Key Components</strong>: <ul> <li>Group normalization for stable training</li> <li>Residual connections for improved gradient flow</li> <li>Feature enhancement blocks for detail preservation</li> </ul> </li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-6/depth_refiner_edges.png" alt="Edge-Aware Processing" style="width: 100%; max-width: 700px;"> <p><em>Edge-aware processing comparing original edges (left) with refined edges (right), demonstrating the preservation of structural details</em></p> </div> <p>The diffusion model was designed to progressively denoise corrupted depth maps while maintaining critical edge information and structural details. This is particularly important for preserving geometric boundaries in complex indoor scenes.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_compute_edge_loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Compute edge-aware loss between prediction and target with robust grid handling.</span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">_get_edges</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">Get edges using Sobel filters with robust grid size handling.</span><span class="sh">"""</span>
        <span class="c1"># Implementation details omitted for brevity
</span>        <span class="n">sobel_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> 
                            <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> 
                            <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
        <span class="n">sobel_y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                            <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                            <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span>
        
        <span class="c1"># Apply filters and compute edge magnitudes
</span>        <span class="n">edges_x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sobel_x</span><span class="p">)</span>
        <span class="n">edges_y</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sobel_y</span><span class="p">)</span>
        <span class="n">edges</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">edges_x</span><span class="p">.</span><span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">edges_y</span><span class="p">.</span><span class="nf">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">edges</span>
    
    <span class="c1"># Compute losses on extracted edges
</span>    <span class="n">pred_edges</span> <span class="o">=</span> <span class="nf">_get_edges</span><span class="p">(</span><span class="n">pred</span><span class="p">)</span>
    <span class="n">target_edges</span> <span class="o">=</span> <span class="nf">_get_edges</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    <span class="n">edge_loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">l1_loss</span><span class="p">(</span><span class="n">pred_edges</span><span class="p">,</span> <span class="n">target_edges</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">edge_loss</span>
</code></pre></div></div> <h4 id="22-memory-efficient-nerf-implementation">2.2 Memory-Efficient NeRF Implementation</h4> <p>The second stage leverages a custom Neural Radiance Field (NeRF) implementation with several optimizations:</p> <ul> <li> <strong>Efficient Encoding</strong>: Implemented hash encoding for faster training and better detail preservation</li> <li> <strong>Performance Optimizations</strong>: <ul> <li>Occupancy grid acceleration to skip empty space</li> <li>Gradient checkpointing to reduce memory usage</li> <li>Mixed precision training for improved throughput</li> </ul> </li> <li> <strong>Advanced Training</strong>: Integrated depth supervision from the diffusion model with custom loss functions</li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-6/depth_analysis.png" alt="Depth Analysis" style="width: 100%; max-width: 1000px;"> <p><em>Depth value distribution analysis showing the distribution of depth values across the dataset, helping to calibrate model parameters</em></p> </div> <p>The NeRF implementation features a sophisticated volumetric rendering approach:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">render_rays</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">rays_o</span><span class="p">,</span> <span class="n">rays_d</span><span class="p">,</span> <span class="n">near</span><span class="p">,</span> <span class="n">far</span><span class="p">,</span> <span class="n">depth_prior</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Volumetric rendering for a batch of rays.</span><span class="sh">"""</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">set_grad_enabled</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">training</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">rays_o</span><span class="p">.</span><span class="n">device</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_rays</span> <span class="o">=</span> <span class="n">rays_o</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
        
        <span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">1024</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_rays</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">):</span>
            <span class="n">chunk_rays_o</span> <span class="o">=</span> <span class="n">rays_o</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">chunk_size</span><span class="p">]</span>
            <span class="n">chunk_rays_d</span> <span class="o">=</span> <span class="n">rays_d</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">chunk_size</span><span class="p">]</span>
            <span class="n">chunk_out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_render_rays_chunk</span><span class="p">(</span>
                <span class="n">model</span><span class="p">,</span> <span class="n">chunk_rays_o</span><span class="p">,</span> <span class="n">chunk_rays_d</span><span class="p">,</span> <span class="n">near</span><span class="p">,</span> <span class="n">far</span><span class="p">,</span> <span class="n">depth_prior</span>
            <span class="p">)</span>
            <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">chunk_out</span><span class="p">)</span>
        <span class="n">combined</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">keys</span><span class="p">():</span>
            <span class="n">combined</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">out</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">combined</span>
</code></pre></div></div> <p>This implementation includes advanced sampling techniques to efficiently distribute samples along each ray:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample_pdf</span><span class="p">(</span><span class="n">bins</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">det</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Sample points from probability distribution given by weights.
    
    This is for sampling points according to their importance during ray marching.
    PDF here means </span><span class="sh">"</span><span class="s">Probability Density Function</span><span class="sh">"</span><span class="s"> which helps focus samples 
    where they</span><span class="sh">'</span><span class="s">re most needed.
    </span><span class="sh">"""</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">.</span><span class="nf">float</span><span class="p">()</span>
    <span class="n">bins</span> <span class="o">=</span> <span class="n">bins</span><span class="p">.</span><span class="nf">float</span><span class="p">()</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">+</span> <span class="mf">1e-5</span>
    <span class="n">pdf</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="n">cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">pdf</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [..., n_bins]
</span>    <span class="n">cdf</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">cdf</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]),</span> <span class="n">cdf</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [..., n_bins+1]
</span>
    <span class="k">if</span> <span class="n">det</span><span class="p">:</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">weights</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">u</span><span class="p">.</span><span class="nf">expand</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">cdf</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="p">[</span><span class="n">n_samples</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">cdf</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="p">[</span><span class="n">n_samples</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">weights</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">u</span> <span class="o">=</span> <span class="n">u</span><span class="p">.</span><span class="nf">contiguous</span><span class="p">()</span>
    <span class="n">cdf</span> <span class="o">=</span> <span class="n">cdf</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="o">*</span><span class="n">cdf</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">cdf</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">inds</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">searchsorted</span><span class="p">(</span><span class="n">cdf</span><span class="p">[...,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">u</span><span class="p">)</span>
    <span class="n">below</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">inds</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">above</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">inds</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="n">cdf</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">inds_g</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">below</span><span class="p">,</span> <span class="n">above</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">bins_expanded</span> <span class="o">=</span> <span class="n">bins</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">).</span><span class="nf">expand</span><span class="p">(</span><span class="o">*</span><span class="n">bins</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">bins</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="n">bins_g</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">bins_expanded</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">inds_g</span><span class="p">)</span>
    <span class="n">cdf_g</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">gather</span><span class="p">(</span><span class="n">cdf</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">inds_g</span><span class="p">)</span>

    <span class="n">denom</span> <span class="o">=</span> <span class="n">cdf_g</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">cdf_g</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">denom</span> <span class="o">&lt;</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">denom</span><span class="p">),</span> <span class="n">denom</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="n">cdf_g</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">denom</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">bins_g</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">t</span> <span class="o">*</span> <span class="p">(</span><span class="n">bins_g</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">bins_g</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">samples</span>
</code></pre></div></div> <hr> <h3 id="3-training-pipeline">3. Training Pipeline</h3> <p>The project includes a comprehensive training pipeline with several advanced techniques:</p> <h4 id="31-diffusion-model-training">3.1 Diffusion Model Training</h4> <ul> <li> <strong>Scheduler</strong>: Implemented OneCycleLR with warmup for stable convergence</li> <li> <strong>Loss Function</strong>: Combined MSE, edge-aware loss, and perceptual loss</li> <li> <strong>Optimization</strong>: Early stopping with patience for optimal model selection</li> <li> <strong>Augmentation</strong>: Designed multi-view consistency loss for better generalization</li> </ul> <h4 id="32-nerf-training">3.2 NeRF Training</h4> <ul> <li> <strong>Custom Loss Functions</strong>: Depth-aware losses with edge preservation terms</li> <li> <strong>Gradient Management</strong>: Implemented gradient clipping and dynamic batch sizing</li> <li> <strong>Memory Optimization</strong>: Integrated various techniques to reduce memory footprint: <ul> <li>Ray chunking for processing large scenes</li> <li>Efficient data loading pipelines</li> <li>Gradient checkpointing to reduce memory during backpropagation</li> </ul> </li> </ul> <div style="text-align: center;"> <div style="display: flex; justify-content: center; gap: 20px; margin-bottom: 20px;"> <img src="/assets/img/project-6/sample_1_detailed.png" alt="Depth Refinement Sample 2" style="width: 45%; max-width: 400px;"> <img src="/assets/img/project-6/sample_2_detailed.png" alt="Depth Refinement Sample 3" style="width: 45%; max-width: 400px;"> </div> <p><em>Additional examples of depth map refinement showing the model's ability to handle different scene types</em></p> </div> <hr> <h3 id="4-results-and-performance">4. Results and Performance</h3> <p>While this project is ongoing, initial results show promising performance:</p> <ul> <li> <strong>Depth Refinement Metrics</strong>: <ul> <li>MSE: 0.8901</li> <li>PSNR: 0.7254</li> </ul> </li> <li> <strong>Memory Efficiency</strong>: Successfully processes high-resolution inputs with limited GPU memory</li> <li> <strong>Edge Preservation</strong>: Significantly improved edge preservation compared to baseline methods</li> </ul> <p>The current implementation demonstrates the system’s ability to effectively:</p> <ol> <li>Denoise and refine depth maps from consumer-grade depth cameras</li> <li>Preserve critical structural information during the denoising process</li> <li>Build consistent 3D reconstructions from the refined depth maps</li> <li>Operate with reasonable memory requirements despite the complexity of the models</li> </ol> <hr> <h3 id="5-future-directions">5. Future Directions</h3> <p>The project has several planned enhancements:</p> <ul> <li> <strong>InstantNGP Techniques</strong>: Integrating additional techniques from InstantNGP for faster rendering</li> <li> <strong>Sparse Voxel Grid Acceleration</strong>: Adding sparse voxel grid for further speed improvements</li> <li> <strong>Interactive Viewer</strong>: Developing an interactive 3D viewer for real-time exploration</li> <li> <strong>Multi-View Consistency</strong>: Enhancing the consistency between multiple viewpoints</li> </ul> <hr> <h3 id="6-project-repository">6. Project Repository</h3> <p><a href="https://github.com/Srecharan/DiffusionNeRF-3D.git" rel="external nofollow noopener" target="_blank">DiffusionNeRF-3D</a></p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Srecharan Selvam. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-repositories",title:"Repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-portfolio",title:"Portfolio",description:"A collection of my detailed technical work and projects",section:"Navigation",handler:()=>{window.location.href="/portfolio/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-",title:"",description:"",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-vision-language-action-enhanced-robotic-leaf-grasping",title:"Vision-Language-Action Enhanced Robotic Leaf Grasping",description:"A novel vision system combining computer vision, deep learning, and Vision-Language-Action models",section:"Projects",handler:()=>{window.location.href="/projects/1_project_minimal/"}},{id:"projects-deeptrade-ai-multi-model-stock-prediction-with-nlp-amp-automated-trading",title:"DeepTrade AI - Multi-Model Stock Prediction with NLP & Automated Trading",description:"An enterprise-grade system integrating LSTM-XGBoost prediction with sentiment analysis, distributed training infrastructure, and automated workflows",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-real-time-hand-gesture-recognition-for-ar-interaction",title:"Real-time Hand Gesture Recognition for AR Interaction",description:"A sophisticated system combining computer vision and deep learning for hand tracking and gesture recognition in augmented reality",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-multi-camera-vision-system-for-automated-material-detection-and-sorting",title:"Multi-Camera Vision System for Automated Material Detection and Sorting",description:"A real-time computer vision system for material recovery and worker safety monitoring on industrial conveyor belts",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-",title:"",description:"",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-genai-for-synthetic-data-augmentation",title:"GenAI for Synthetic Data Augmentation",description:"Validating synthetic data augmentation for computer vision with measurable accuracy improvements",section:"Projects",handler:()=>{window.location.href="/projects/5_project_minimal/"}},{id:"projects-high-fidelity-3d-scene-reconstruction-integrating-diffusion-models-with-memory-efficient-neural-radiance-fields",title:"High-Fidelity 3D Scene Reconstruction Integrating Diffusion Models with Memory-Efficient Neural Radiance Fields",description:"A novel approach combining diffusion models with Neural Radiance Fields for high-quality 3D scene reconstruction",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-safetyvlm-vlm-based-tool-recognition-system-for-industrial-safety-applications",title:"SafetyVLM: VLM-Based Tool Recognition System for Industrial Safety Applications",description:"A comprehensive system for tool recognition and safety guidance using fine-tuned vision-language models with LangChain RAG, Pinecone, Docker, and Kubernetes deployment",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%73%65%6C%76%61%6D@%61%6E%64%72%65%77.%63%6D%75.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=Srecharan Selvam","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Srecharan","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/srecharan","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>