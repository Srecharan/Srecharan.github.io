<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> ToolVisionLM: Enhancing Vision-Language Models for Industrial Safety [Ongoing] | Srecharan Selvam </title> <meta name="author" content="Srecharan Selvam"> <meta name="description" content="A comprehensive evaluation framework for vision-language models in technical domains with focus on industrial tool recognition and safety guidance"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon2.png?f718b847644d9f9675c29095ed121769"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://srecharan.github.io/projects/7_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Srecharan Selvam </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/portfolio/">Portfolio </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">ToolVisionLM: Enhancing Vision-Language Models for Industrial Safety [Ongoing]</h1> <p class="post-description">A comprehensive evaluation framework for vision-language models in technical domains with focus on industrial tool recognition and safety guidance</p> </header> <article> <h3 id="1-overview">1. Overview</h3> <p>ToolVisionLM is an innovative research project that explores the application of Vision-Language Models (VLMs) to specialized technical domains, with a focus on industrial tool recognition, usage instruction, and safety guidance. While VLMs have demonstrated remarkable capabilities in general visual understanding tasks, their application to specialized domains remains limited. This project addresses this gap by developing a comprehensive evaluation framework for assessing VLM performance in industrial settings where proper tool handling directly impacts workplace safety and operational efficiency.</p> <div style="text-align: center;"> <img src="/assets/img/project-7/VLM.png" alt="Vision-Language Model Architecture" style="width: 90%; max-width: 800px;"> <p><em>General architecture of vision-language models for tool recognition showing the image encoding, vision-language fusion, and task-specific outputs</em></p> </div> <hr> <h3 id="2-technical-approach">2. Technical Approach</h3> <h4 id="21-dataset-preparation">2.1 Dataset Preparation</h4> <p>The project features a meticulously curated dataset spanning 13 core tool categories:</p> <ul> <li> <strong>Primary Tools</strong>: Wrenches, hammers, pliers, screwdrivers (most common industrial tools)</li> <li> <strong>Secondary Tools</strong>: Bolts, dynamometers, testers, tool boxes</li> <li> <strong>Measurement Tools</strong>: Tape measures, calipers</li> <li> <strong>Power/Cutting Tools</strong>: Ratchets, drills, saws</li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-7/data_graph.png" alt="Dataset Distribution" style="width: 90%; max-width: 800px;"> <p><em>Distribution of industrial tool categories across the three datasets used in this project, showing the comprehensive coverage across tool types</em></p> </div> <p>Data aggregation was performed across three distinct sources to ensure comprehensive coverage:</p> <ol> <li>A specialized tool dataset with fine-grained classifications (Dataset 1)</li> <li>A general tool dataset with broad category coverage (Dataset 2)</li> <li>A high-quality dataset with diverse tool representations (Dataset 3)</li> </ol> <p>This consolidation produced a balanced dataset with over 20 images per tool category (minimum) and more than 1,000 total images, enabling robust model training and evaluation.</p> <h4 id="22-model-selection">2.2 Model Selection</h4> <p>The project evaluates several state-of-the-art Vision-Language Models to benchmark their performance on specialized tool recognition tasks:</p> <ul> <li> <strong>Qwen2-VL-7B-Instruct</strong>: Alibaba’s 7B parameter instruction-tuned multimodal model</li> <li> <strong>Phi-3-vision-128k-instruct</strong>: Microsoft’s vision-capable model with extended context window</li> <li> <strong>Llama-3.2-11B-Vision-Instruct</strong>: Meta’s 11B parameter multimodal model</li> <li> <strong>SmolVLM</strong>: A lightweight VLM optimized for efficient deployment</li> <li> <strong>PaliGemma</strong>: Google’s multimodal model built on the Gemma architecture</li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-7/flowchart.png" alt="Project Pipeline" style="width: 75%; max-width: 700px;"> <p><em>ToolVisionLM pipeline showing dataset preparation, model selection, parallel approach strategies, and evaluation methodology</em></p> </div> <h4 id="23-dual-enhancement-approach">2.3 Dual Enhancement Approach</h4> <p>The project implements two parallel enhancement strategies to optimize VLM performance for tool-related tasks:</p> <h5 id="fine-tuning-strategy">Fine-tuning Strategy</h5> <ul> <li>Dataset-specific model adaptation using controlled fine-tuning procedures</li> <li>Parameter-efficient fine-tuning techniques to preserve general capabilities</li> <li>Domain-specific prompt engineering optimized for tool recognition tasks</li> <li>Balanced training across all tool categories to prevent bias</li> </ul> <h5 id="rag-retrieval-augmented-generation-approach">RAG (Retrieval-Augmented Generation) Approach</h5> <ul> <li>Development of a specialized knowledge base containing detailed tool specifications</li> <li>Implementation of efficient embedding and retrieval mechanisms</li> <li>Context-aware information retrieval to enhance model responses</li> <li>Hybrid retrieval approaches combining visual and textual information</li> </ul> <hr> <h3 id="3-evaluation-framework">3. Evaluation Framework</h3> <p>The evaluation methodology incorporates multiple dimensions to provide a comprehensive assessment of model performance:</p> <h4 id="31-recognition-accuracy">3.1 Recognition Accuracy</h4> <ul> <li> <strong>Precision, Recall, and F1 Scores</strong>: Traditional metrics for identification accuracy</li> <li> <strong>Cross-Category Confusion Analysis</strong>: Identifying common misclassification patterns</li> <li> <strong>Challenging Scenario Testing</strong>: Performance under occlusion, unusual angles, and poor lighting</li> </ul> <h4 id="32-instruction-quality">3.2 Instruction Quality</h4> <ul> <li> <strong>Completeness</strong>: Evaluating whether responses include all critical information</li> <li> <strong>Correctness</strong>: Assessing technical accuracy of usage instructions</li> <li> <strong>Clarity</strong>: Measuring how understandable the instructions are for end-users</li> </ul> <h4 id="33-safety-guidance">3.3 Safety Guidance</h4> <ul> <li> <strong>Safety-Critical Information</strong>: Identifying presence of essential safety warnings</li> <li> <strong>Hazard Recognition</strong>: Evaluating model’s ability to identify potential dangers</li> <li> <strong>Protective Equipment Recommendations</strong>: Checking for appropriate safety gear suggestions</li> </ul> <p>The framework employs both automated metrics and targeted qualitative analysis to identify strengths, weaknesses, and potential improvement areas for each model.</p> <hr> <h3 id="4-preliminary-findings">4. Preliminary Findings</h3> <p>While the project is ongoing, initial explorations have yielded several promising insights:</p> <ul> <li>VLMs demonstrate varying capabilities in recognizing tool categories, with performance generally correlating with model size</li> <li>Fine-grained tool identification (distinguishing subtypes like Phillips vs. flat-head screwdrivers) remains challenging for most models</li> <li>All models show significant improvements when augmented with either fine-tuning or RAG approaches</li> <li>Safety guidance quality varies substantially, with larger models providing more comprehensive safety information</li> <li>The RAG approach shows particular promise for enhancing safety instruction accuracy without extensive model adaptation</li> </ul> <hr> <h3 id="5-applications-and-impact">5. Applications and Impact</h3> <p>This research has significant implications for several industrial applications:</p> <ul> <li> <strong>Safety Training</strong>: Enhanced VLMs could provide on-demand tool usage guidance in industrial settings</li> <li> <strong>Maintenance Support</strong>: Interactive systems could assist technicians with proper tool selection and usage</li> <li> <strong>Quality Assurance</strong>: Automated systems could verify correct tool usage in manufacturing processes</li> <li> <strong>Accessibility</strong>: Improved visual recognition systems could make technical work more accessible</li> </ul> <p>By addressing the limitations of current VLMs in specialized domains, this project aims to bridge the gap between general visual understanding and domain-specific technical knowledge, ultimately enhancing workplace safety and efficiency.</p> <hr> <h3 id="6-future-directions">6. Future Directions</h3> <p>As the project progresses, several promising avenues for future work have been identified:</p> <ul> <li>Expanding the tool dataset to include more specialized industrial categories and rare tools</li> <li>Incorporating multimodal feedback mechanisms to improve instruction clarity and correctness</li> <li>Developing benchmark datasets for other technical domains using similar methodologies</li> <li>Exploring lightweight deployment options for resource-constrained industrial environments</li> <li>Integrating real-time safety monitoring capabilities into the system</li> </ul> <hr> <h3 id="7-project-repository">7. Project Repository</h3> <p><a href="https://github.com/Srecharan/ToolVisionLM" rel="external nofollow noopener" target="_blank">ToolVisionLM on GitHub</a></p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Srecharan Selvam. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-repositories",title:"Repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-portfolio",title:"Portfolio",description:"A collection of my detailed technical work and projects",section:"Navigation",handler:()=>{window.location.href="/portfolio/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-hybrid-cv-ml-approach-for-autonomous-leaf-grasping",title:"Hybrid CV-ML Approach for Autonomous Leaf Grasping",description:"A novel vision system combining geometric computer vision with deep learning for leaf detection and grasp point optimization",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-deeptrade-ai-multi-model-stock-prediction-with-nlp-amp-automated-trading",title:"DeepTrade AI - Multi-Model Stock Prediction with NLP & Automated Trading",description:"An end-to-end system integrating LSTM-XGBoost prediction with sentiment analysis for automated trading",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-real-time-hand-gesture-recognition-for-ar-interaction",title:"Real-time Hand Gesture Recognition for AR Interaction",description:"A sophisticated system combining computer vision and deep learning for hand tracking and gesture recognition in augmented reality",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-multi-camera-vision-system-for-automated-material-detection-and-sorting",title:"Multi-Camera Vision System for Automated Material Detection and Sorting",description:"A real-time computer vision system for material recovery and worker safety monitoring on industrial conveyor belts",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-deep-image-synthesis-with-gans-vaes-and-diffusion-models",title:"Deep Image Synthesis with GANs, VAEs, and Diffusion Models",description:"A comprehensive implementation of three leading generative model architectures for image synthesis",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-high-fidelity-3d-scene-reconstruction-integrating-diffusion-models-with-memory-efficient-neural-radiance-fields",title:"High-Fidelity 3D Scene Reconstruction Integrating Diffusion Models with Memory-Efficient Neural Radiance Fields",description:"A novel approach combining diffusion models with Neural Radiance Fields for high-quality 3D scene reconstruction",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-toolvisionlm-enhancing-vision-language-models-for-industrial-safety-ongoing",title:"ToolVisionLM: Enhancing Vision-Language Models for Industrial Safety [Ongoing]",description:"A comprehensive evaluation framework for vision-language models in technical domains with focus on industrial tool recognition and safety guidance",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%73%65%6C%76%61%6D@%61%6E%64%72%65%77.%63%6D%75.%65%64%75","_blank")}},{id:"socials-whatsapp",title:"WhatsApp",section:"Socials",handler:()=>{window.open("https://wa.me/19174953410","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=Srecharan Selvam","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Srecharan","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/srecharan@gmail.com","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>