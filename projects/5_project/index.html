<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Deep Image Synthesis with GANs, VAEs, and Diffusion Models | Srecharan Selvam </title> <meta name="author" content="Srecharan Selvam"> <meta name="description" content="A comprehensive implementation of three leading generative model architectures for image synthesis"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?29f7cbda2ac1020cfcef4c86b03c404d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://srecharan.github.io/projects/5_project/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Srecharan Selvam </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"><i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Deep Image Synthesis with GANs, VAEs, and Diffusion Models</h1> <p class="post-description">A comprehensive implementation of three leading generative model architectures for image synthesis</p> </header> <article> <h3 id="1-overview">1. Overview</h3> <p>This project explores three fundamentally different approaches to deep generative modeling for image synthesis, implementing Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Diffusion Models from scratch. Each architecture was built, trained, and evaluated on the challenging CUB-200-2011 bird dataset, which requires models to capture subtle visual features and details. The implementation includes custom architectures, loss functions, and training strategies, with rigorous evaluation using the Fréchet Inception Distance (FID) metric to quantitatively assess the quality of generated images. The project demonstrates the effectiveness of different generative approaches, with WGAN-GP emerging as the strongest performer, achieving an FID score of 33.07.</p> <hr> <h3 id="2-generative-adversarial-networks-gans">2. Generative Adversarial Networks (GANs)</h3> <p>The exploration began with implementing and training three distinct GAN variants, each with progressively improved stability and performance:</p> <h4 id="21-architecture-design">2.1 Architecture Design</h4> <p>The GAN implementation features custom architectures for both generator and discriminator:</p> <ul> <li> <strong>Generator</strong>: Takes a 128-dimensional noise vector and progressively upsamples it through custom ResBlockUp modules, which improve gradient flow and spatial resolution, outputting a 3×32×32 image</li> <li> <strong>Discriminator</strong>: Processes the 3×32×32 image through ResBlockDown modules and standard ResBlocks for feature extraction, producing a scalar output representing authenticity</li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-5/gan_figure.png" alt="GAN Architecture" style="width: 50%; max-width: 500px;"> <p><em>GAN architecture showing generator and discriminator networks with ResBlock components</em></p> </div> <h4 id="22-gan-variants-and-loss-functions">2.2 GAN Variants and Loss Functions</h4> <p>Three different GAN variants were implemented and trained from scratch, each with its unique loss function and training dynamics:</p> <h5 id="221-vanilla-gan">2.2.1 Vanilla GAN</h5> <p>The original GAN formulation was implemented using Binary Cross-Entropy loss. The generator was trained to minimize the log-probability of the discriminator correctly identifying fake images, while the discriminator was trained to maximize the log-probability of correct classification.</p> <p>Training this model revealed the challenges of the original GAN approach, including significant training instability and mode collapse. Despite these issues, the model managed to learn basic bird shapes and features, achieving an FID score of 104.62.</p> <div style="text-align: center;"> <div style="display: flex; justify-content: center; gap: 20px; margin-bottom: 20px;"> <img src="/assets/img/project-5/Vanilla_GAN_Samples.png" alt="Vanilla GAN Samples" style="width: 45%; max-width: 400px;"> <img src="/assets/img/project-5/Vanilla%20GAN%20Latent%20Space%20Interpolations.png" alt="Vanilla GAN Interpolations" style="width: 45%; max-width: 400px;"> </div> <p><em>Left: Vanilla GAN samples. Right: Latent space interpolations showing transitions between generated images</em></p> </div> <h5 id="222-least-squares-gan-lsgan">2.2.2 Least Squares GAN (LSGAN)</h5> <p>Building upon the Vanilla GAN, the LSGAN replaced the sigmoid cross-entropy with a least-squares loss, minimizing the squared difference between the discriminator’s output and target labels. This modification produced more stable training dynamics and improved sample diversity.</p> <p>The LSGAN implementation successfully reduced training instability and generated more diverse and realistic bird images, with an FID score of 52.48, a significant improvement over the Vanilla GAN.</p> <div style="text-align: center;"> <div style="display: flex; justify-content: center; gap: 20px; margin-bottom: 20px;"> <img src="/assets/img/project-5/LS-GAN%20Samples.png" alt="LSGAN Samples" style="width: 45%; max-width: 400px;"> <img src="/assets/img/project-5/LS-GAN%20Latent%20Space%20Interpolations.png" alt="LSGAN Interpolations" style="width: 45%; max-width: 400px;"> </div> <p><em>Left: LSGAN samples showing improved diversity. Right: Smooth latent space interpolations</em></p> </div> <h5 id="223-wasserstein-gan-with-gradient-penalty-wgan-gp">2.2.3 Wasserstein GAN with Gradient Penalty (WGAN-GP)</h5> <p>The most sophisticated implementation utilized the Wasserstein distance with gradient penalty for enforcing the 1-Lipschitz constraint. The discriminator (now a “critic”) was trained to estimate the Wasserstein distance between real and generated distributions.</p> <p>A key innovation in this implementation was the gradient penalty term, calculated by sampling interpolated points between real and fake images and penalizing deviations of the gradient norm from 1. This approach demonstrated the most stable training and generated the highest quality images, with an FID score of 33.07.</p> <div style="text-align: center;"> <div style="display: flex; justify-content: center; gap: 20px; margin-bottom: 20px;"> <img src="/assets/img/project-5/WGAN-GP%20Samples.png" alt="WGAN-GP Samples" style="width: 45%; max-width: 400px;"> <img src="/assets/img/project-5/WGAN-GP%20Latent%20Space%20Interpolations.png" alt="WGAN-GP Interpolations" style="width: 45%; max-width: 400px;"> </div> <p><em>Left: WGAN-GP samples showing the highest quality. Right: Highly coherent latent space interpolations</em></p> </div> <hr> <h3 id="3-variational-autoencoders-vaes">3. Variational Autoencoders (VAEs)</h3> <p>The second phase focused on building and training Variational Autoencoders, exploring their unique ability to learn structured latent representations while balancing reconstruction quality and sampling capability.</p> <h4 id="31-vae-architecture">3.1 VAE Architecture</h4> <p>A standard VAE architecture was implemented with several key components:</p> <ul> <li> <strong>Encoder</strong>: A convolutional network that maps input images to a distribution in latent space, represented by mean (μ) and log standard deviation (log σ) vectors</li> <li> <strong>Latent Space</strong>: Implemented with the reparameterization trick (z = μ + σ * ε, where ε ~ N(0,1)) to enable backpropagation through the sampling process</li> <li> <strong>Decoder</strong>: A network of transposed convolutions that reconstructs images from latent vectors</li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-5/vae_figure.png" alt="VAE Architecture" style="width: 50%; max-width: 500px;"> <p><em>VAE architecture showing encoder, latent space with reparameterization, and decoder components</em></p> </div> <h4 id="32-latent-space-experiments">3.2 Latent Space Experiments</h4> <p>Experiments were conducted with different latent space dimensions to explore the trade-offs:</p> <ul> <li> <strong>Latent Size 16</strong>: Limited capacity resulted in blurry reconstructions with high reconstruction loss (~200)</li> <li> <strong>Latent Size 128</strong>: Provided a balance between reconstruction quality and sample diversity with moderate loss (~75)</li> <li> <strong>Latent Size 1024</strong>: Achieved the sharpest reconstructions with lowest loss (~40) but potentially less structured latent space</li> </ul> <div style="text-align: center;"> <div style="display: flex; justify-content: center; gap: 20px; margin-bottom: 20px;"> <img src="/assets/img/project-5/Reconstructions:%20(size%2016).png" alt="Reconstructions Size 16" style="width: 30%; max-width: 275px;"> <img src="/assets/img/project-5/Reconstructions:%20(size%20128).png" alt="Reconstructions Size 128" style="width: 30%; max-width: 275px;"> <img src="/assets/img/project-5/Reconstructions:%20(size%201024).png" alt="Reconstructions Size 1024" style="width: 30%; max-width: 275px;"> </div> <p><em>Reconstructions with increasing latent dimensions (16, 128, 1024), showing improved quality with larger latent spaces</em></p> </div> <h4 id="33-β-vae-and-annealing">3.3 β-VAE and Annealing</h4> <p>To optimize the balance between reconstruction accuracy and latent space regularity, the following techniques were implemented and trained:</p> <ul> <li> <strong>β Parameter Control</strong>: Different values of β (0.8, 1.0, 1.2) were investigated, which controls the weight of the KL divergence term in the loss function</li> <li> <strong>β-Annealing</strong>: A linear annealing schedule was implemented that gradually increased β from 0 to the target value (0.8) over 20 epochs</li> </ul> <div style="text-align: center;"> <div style="display: flex; justify-content: center; gap: 20px; margin-bottom: 20px;"> <img src="/assets/img/project-5/constant_beta_samples.png" alt="Constant Beta Samples" style="width: 45%; max-width: 400px;"> <img src="/assets/img/project-5/annelaed_beta.png" alt="Annealed Beta Samples" style="width: 45%; max-width: 400px;"> </div> <p><em>Left: Samples with constant β=0.8. Right: Samples after β-annealing, showing improved quality and diversity</em></p> </div> <hr> <h3 id="4-diffusion-models">4. Diffusion Models</h3> <p>The final phase focused on implementing Diffusion Models, specifically investigating sampling strategies for a pre-trained model.</p> <h4 id="41-diffusion-architecture">4.1 Diffusion Architecture</h4> <p>The diffusion model implementation focused on the inference process, using a pre-trained U-Net backbone:</p> <ul> <li> <strong>Forward Process</strong>: A fixed process that sequentially adds Gaussian noise to images over T timesteps</li> <li> <strong>Reverse Process</strong>: A learned denoising process that iteratively removes noise, using the U-Net to predict the noise component at each step</li> </ul> <div style="text-align: center;"> <img src="/assets/img/project-5/diffusion_figure.png" alt="Diffusion Model" style="width: 50%; max-width: 500px;"> <p><em>Diffusion model architecture showing forward noising process and learned reverse denoising process</em></p> </div> <h4 id="42-sampling-strategies">4.2 Sampling Strategies</h4> <p>Two sampling approaches were implemented and thoroughly tested:</p> <ul> <li> <p><strong>DDPM (Denoising Diffusion Probabilistic Models)</strong>: The original sampling approach requiring approximately 1000 sequential denoising steps. This produced high-quality samples with an FID score of 34.73.</p> </li> <li> <p><strong>DDIM (Denoising Diffusion Implicit Models)</strong>: An accelerated sampling approach using a non-Markovian diffusion process, requiring only 100 steps. This maintained comparable quality with an FID score of 38.32, demonstrating a significant efficiency improvement.</p> </li> </ul> <div style="text-align: center;"> <div style="display: flex; justify-content: center; gap: 20px; margin-bottom: 20px;"> <img src="/assets/img/project-5/DDPM%20Samples.png" alt="DDPM Samples" style="width: 45%; max-width: 400px;"> <img src="/assets/img/project-5/DDIM%20Samples.png" alt="DDIM Samples" style="width: 45%; max-width: 400px;"> </div> <p><em>Left: DDPM samples (1000 steps). Right: DDIM samples (100 steps), showing comparable quality with 10× faster sampling</em></p> </div> <hr> <h3 id="5-performance-evaluation">5. Performance Evaluation</h3> <p>A comprehensive quantitative evaluation was conducted using the Fréchet Inception Distance (FID) to measure the quality and diversity of generated images:</p> <div style="text-align: center;"> <table class="table" style="width: 80%; margin: 0 auto; border-collapse: collapse; border: 1px solid #ddd;"> <thead> <tr style="background-color: #f2f2f2;"> <th style="padding: 12px; border: 1px solid #ddd;">Model</th> <th style="padding: 12px; border: 1px solid #ddd;">FID Score</th> <th style="padding: 12px; border: 1px solid #ddd;">Training Stability</th> <th style="padding: 12px; border: 1px solid #ddd;">Sampling Speed</th> </tr> </thead> <tbody> <tr> <td style="padding: 12px; border: 1px solid #ddd;">Vanilla GAN</td> <td style="padding: 12px; border: 1px solid #ddd;">104.62</td> <td style="padding: 12px; border: 1px solid #ddd;">Unstable</td> <td style="padding: 12px; border: 1px solid #ddd;">Fast</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 12px; border: 1px solid #ddd;">LSGAN</td> <td style="padding: 12px; border: 1px solid #ddd;">52.48</td> <td style="padding: 12px; border: 1px solid #ddd;">More Stable</td> <td style="padding: 12px; border: 1px solid #ddd;">Fast</td> </tr> <tr> <td style="padding: 12px; border: 1px solid #ddd;">WGAN-GP</td> <td style="padding: 12px; border: 1px solid #ddd;">33.07</td> <td style="padding: 12px; border: 1px solid #ddd;">Stable</td> <td style="padding: 12px; border: 1px solid #ddd;">Fast</td> </tr> <tr style="background-color: #f9f9f9;"> <td style="padding: 12px; border: 1px solid #ddd;">DDPM</td> <td style="padding: 12px; border: 1px solid #ddd;">34.73</td> <td style="padding: 12px; border: 1px solid #ddd;">Stable</td> <td style="padding: 12px; border: 1px solid #ddd;">Slow (1000 steps)</td> </tr> <tr> <td style="padding: 12px; border: 1px solid #ddd;">DDIM</td> <td style="padding: 12px; border: 1px solid #ddd;">38.32</td> <td style="padding: 12px; border: 1px solid #ddd;">Stable</td> <td style="padding: 12px; border: 1px solid #ddd;">Medium (100 steps)</td> </tr> </tbody> </table> </div> <p><strong>Key Findings:</strong></p> <ul> <li>WGAN-GP emerged as the best-performing model with an FID score of 33.07</li> <li>Diffusion models (DDPM) produced comparable quality to WGAN-GP but required significantly more sampling steps</li> <li>VAEs with β-annealing showed improved sample quality, though quantitative comparison was not performed</li> <li>Training stability improved considerably from Vanilla GAN to LSGAN to WGAN-GP</li> </ul> <hr> <h3 id="6-key-contributions">6. Key Contributions</h3> <p>This independent research project was completed as an individual endeavor, with significant technical contributions including:</p> <ul> <li>Implementation and training of three different GAN architectures from scratch with custom ResBlock components and loss functions</li> <li>Development and training of the gradient penalty mechanism for WGAN-GP to enforce the 1-Lipschitz constraint</li> <li>Building and training VAEs with exploration of latent space dimensions and implementation of β-annealing for improved sample quality</li> <li>Implementation of both DDPM and DDIM sampling strategies for diffusion models</li> <li>Evaluation and analysis of different generative architectures on the same dataset</li> </ul> <hr> <h3 id="7-technologies--skills-used">7. Technologies &amp; Skills Used</h3> <ul> <li> <strong>Languages &amp; Frameworks</strong>: Python, PyTorch, TensorFlow, NumPy, OpenCV, scikit-learn</li> <li> <strong>Deep Learning</strong>: Generative Adversarial Networks, Variational Autoencoders, Diffusion Models, Model Training, Hyperparameter Tuning</li> <li> <strong>Loss Functions</strong>: Adversarial Loss, Reconstruction Loss, KL Divergence, Gradient Penalty</li> <li> <strong>Computer Vision</strong>: Image Synthesis, Image Generation, Feature Visualization</li> </ul> <hr> <h3 id="8-project-repository">8. Project Repository</h3> <p><a href="https://github.com/Srecharan/GenVision.git" rel="external nofollow noopener" target="_blank">GenVision</a></p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Srecharan Selvam. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-repositories",title:"Repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-hybrid-cv-ml-approach-for-autonomous-leaf-grasping",title:"Hybrid CV-ML Approach for Autonomous Leaf Grasping",description:"A novel vision system combining geometric computer vision with deep learning for leaf detection and grasp point optimization",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-deeptrade-ai-multi-model-stock-prediction-with-nlp-amp-automated-trading",title:"DeepTrade AI - Multi-Model Stock Prediction with NLP & Automated Trading",description:"An end-to-end system integrating LSTM-XGBoost prediction with sentiment analysis for automated trading",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-real-time-hand-gesture-recognition-for-ar-interaction",title:"Real-time Hand Gesture Recognition for AR Interaction",description:"A sophisticated system combining computer vision and deep learning for hand tracking and gesture recognition in augmented reality",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-multi-camera-vision-system-for-automated-material-detection-and-sorting",title:"Multi-Camera Vision System for Automated Material Detection and Sorting",description:"A real-time computer vision system for material recovery and worker safety monitoring on industrial conveyor belts",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-deep-image-synthesis-with-gans-vaes-and-diffusion-models",title:"Deep Image Synthesis with GANs, VAEs, and Diffusion Models",description:"A comprehensive implementation of three leading generative model architectures for image synthesis",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%73%65%6C%76%61%6D@%61%6E%64%72%65%77.%63%6D%75.%65%64%75","_blank")}},{id:"socials-whatsapp",title:"WhatsApp",section:"Socials",handler:()=>{window.open("https://wa.me/19174953410","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=Srecharan Selvam","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/Srecharan","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/srecharan@gmail.com","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>